
---
title: "RL Algorithms Overview"
description: "Deep dive into the reinforcement learning algorithms powering Artemis"
---

Artemis RL Gym implements state-of-the-art reinforcement learning algorithms specifically optimized for Large Language Model training. Each algorithm addresses different aspects of LLM training challenges, from sample efficiency to preference alignment.

## Algorithm Architecture

```mermaid
graph TD
    subgraph PolicyBased ["Policy-Based Methods"]
        GRPO["GRPO<br/>Group Relative Policy Optimization"]
        PPO["PPO<br/>Proximal Policy Optimization"]
        PRISM["PRISM<br/>Predictive Reinforcement via<br/>Integrated Stochastic Modelling"]
    end
    
    subgraph PreferenceBased ["Preference-Based Methods"]
        DPO["DPO<br/>Direct Preference Optimization"]
        RLHF["RLHF<br/>RL from Human Feedback"]
    end
    
    subgraph CoreComponents ["Core Components"]
        AC["Actor-Critic"]
        RB["Reward Modeling"]
        PE["Policy Evaluation"]
        PU["Policy Updates"]
    end
    
    subgraph TrainingPipeline ["Training Pipeline"]
        DATA["Training Data"]
        ENV["Environment"]
        REWARD["Reward Function"]
        MODEL["LLM Model"]
    end
    
    GRPO --> AC
    PPO --> AC
    PRISM --> AC
    DPO --> RB
    RLHF --> RB
    
    AC --> PE
    AC --> PU
    RB --> PE
    
    DATA --> ENV
    ENV --> REWARD
    REWARD --> MODEL
    MODEL --> PE
    
    style GRPO fill:#16A34A,stroke:#15803D,color:#fff
    style DPO fill:#0EA5E9,stroke:#0284C7,color:#fff
    style PPO fill:#8B5CF6,stroke:#7C3AED,color:#fff
    style PRISM fill:#FF6B6B,stroke:#E55555,color:#fff
    style RLHF fill:#F59E0B,stroke:#D97706,color:#fff
```

## Algorithm Comparison

<Tabs>
<Tab title="PRISM">
  **Predictive Reinforcement via Integrated Stochastic Modelling**
  
  Revolutionary model-based RL algorithm with uncertainty-aware exploration and risk-sensitive learning.
  
  **Best for:**
  - Complex planning tasks
  - Sparse reward environments
  - Risk-sensitive applications
  - Sample-limited domains
  
  **Key advantages:**
  - Multi-step predictive planning
  - Uncertainty-guided exploration
  - Risk-adjusted value functions
  - Superior sample efficiency
</Tab>

<Tab title="GRPO">
  **Group Relative Policy Optimization**
  
  Advanced policy gradient method that optimizes relative to multiple reference policies.
  
  **Best for:**
  - Large-scale LLM training
  - Improved sample efficiency
  - Stable convergence
  
  **Key advantages:**
  - Better exploration through group dynamics
  - Reduced variance in policy updates
  - Automatic curriculum learning
</Tab>

<Tab title="DPO">
  **Direct Preference Optimization**
  
  Preference-based method that learns directly from human feedback without reward modeling.
  
  **Best for:**
  - Human preference alignment
  - Safety-critical applications
  - Instruction following
  
  **Key advantages:**
  - No reward model required
  - Direct preference learning
  - Stable training dynamics
</Tab>

<Tab title="PPO">
  **Proximal Policy Optimization**
  
  Reliable baseline algorithm with proven stability and performance.
  
  **Best for:**
  - Baseline comparisons
  - Well-understood domains
  - Resource-constrained settings
  
  **Key advantages:**
  - Well-tested and stable
  - Good general performance
  - Extensive literature support
</Tab>
</Tabs>

## Algorithm Performance Comparison

| Metric | PRISM | GRPO | DPO | PPO |
|--------|-------|------|-----|-----|
| **Sample Efficiency** | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê |
| **Training Stability** | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê |
| **Human Alignment** | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê |
| **Planning Capability** | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê | ‚≠ê | ‚≠ê |
| **Risk Awareness** | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê | ‚≠ê‚≠ê | ‚≠ê |
| **Implementation Complexity** | ‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |
| **Computational Cost** | ‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |

## Core Algorithm Interface

All algorithms in Artemis implement the `BaseRLAlgorithm` interface:

```python
from artemis.rl_algorithms.base_rl import BaseRLAlgorithm, AlgorithmConfig

class BaseRLAlgorithm(ABC):
    @abstractmethod
    def update(self, experience_batch: List[Experience]) -> Dict[str, float]:
        """Update policy based on collected experience"""
        
    @abstractmethod
    def get_action_probabilities(self, observations: List[Observation]) -> torch.Tensor:
        """Get action probability distributions"""
        
    @abstractmethod
    def save_checkpoint(self, path: str) -> None:
        """Save algorithm state"""
        
    @abstractmethod
    def load_checkpoint(self, path: str) -> None:
        """Load algorithm state"""
```

## Featured Algorithm: PRISM

üîÆ **[PRISM (Predictive Reinforcement via Integrated Stochastic Modelling)](/artemis/algorithms/prism)** is Noema Research's breakthrough RL algorithm that combines:

- **üß† Predictive Models**: Learn environment dynamics for multi-step planning
- **üéØ Smart Exploration**: Uncertainty-guided exploration for maximum learning efficiency  
- **‚öñÔ∏è Risk Awareness**: Risk-adjusted value functions for robust decision-making
- **üìà Sample Efficiency**: Superior performance with fewer environment interactions

<Card title="üöÄ Learn PRISM" href="/artemis/algorithms/prism" icon="brain">
  Dive deep into PRISM's mathematical foundation, implementation details, and advanced features. Perfect for researchers and practitioners working on complex RL tasks.
</Card>

---

## Group Relative Policy Optimization (GRPO)

GRPO is Artemis's flagship algorithm, designed specifically for LLM training with superior sample efficiency and stability.

### Core Concept

Instead of optimizing against a single reference policy, GRPO maintains a group of reference policies and optimizes relative to their collective performance:

```python
# Traditional PPO objective
L_PPO = E[min(r_t(Œ∏) * A_t, clip(r_t(Œ∏), 1-Œµ, 1+Œµ) * A_t)]

# GRPO objective with group relative optimization
L_GRPO = E[min(r_t^group(Œ∏) * A_t^group, clip(r_t^group(Œ∏), 1-Œµ_rel, 1+Œµ_rel) * A_t^group)]
```

Where:
- `r_t^group(Œ∏)` is the policy ratio relative to the group average
- `A_t^group` is the group-relative advantage
- `Œµ_rel` is the relative clipping parameter

### Configuration

```python
from artemis.rl_algorithms.grpo import GRPO, GRPOConfig

config = GRPOConfig(
    # GRPO-specific parameters
    num_reference_policies=3,           # Number of reference policies
    reference_weight=0.1,               # Weight of reference policy influence
    kl_penalty_coefficient=0.02,        # KL divergence penalty
    relative_clip_epsilon=0.3,          # Relative clipping parameter
    group_normalization=True,           # Normalize across group
    adaptive_reference_weight=True,     # Adapt reference weight during training
    
    # Standard RL parameters
    learning_rate=1e-4,
    clip_epsilon=0.2,
    value_coefficient=0.5,
    entropy_coefficient=0.01,
    max_grad_norm=1.0,
    batch_size=64,
    epochs_per_update=4
)

grpo = GRPO(config)
```

### Key Features

<AccordionGroup>
<Accordion title="Multi-Reference Policy Learning">
  GRPO maintains multiple reference policies that capture different aspects of the task:

  ```python
  class GRPOReferencePolicyManager:
      def __init__(self, num_policies: int):
          self.reference_policies = []
          self.policy_weights = torch.ones(num_policies) / num_policies
          
      def update_references(self, current_policy, performance_metrics):
          # Update reference policies based on performance
          if performance_metrics['reward'] > self.best_performance:
              self.add_reference_policy(current_policy.copy())
              
      def get_group_baseline(self, observations):
          # Compute group baseline for advantage estimation
          group_values = []
          for policy in self.reference_policies:
              values = policy.get_values(observations)
              group_values.append(values)
          return torch.stack(group_values).mean(dim=0)
  ```
</Accordion>

<Accordion title="Adaptive Reference Weighting">
  The influence of reference policies adapts based on training progress:

  ```python
  def adaptive_reference_weight_schedule(self, episode: int, performance: float):
      # High reference weight early in training for stability
      if episode < 100:
          return 0.5
      # Reduce reference weight as policy improves
      elif performance > 0.8:
          return 0.05
      else:
          return 0.1 * (1.0 - performance)
  ```
</Accordion>

<Accordion title="Group-Relative Advantage Estimation">
  Advantages are computed relative to the group performance:

  ```python
  def compute_group_relative_advantages(self, rewards, values, group_values):
      # Standard advantage
      advantages = rewards - values
      
      # Group-relative component
      group_advantages = rewards - group_values
      
      # Combine with weighting
      final_advantages = (
          (1 - self.reference_weight) * advantages +
          self.reference_weight * group_advantages
      )
      
      return final_advantages
  ```
</Accordion>
</AccordionGroup>

### Usage Example

```python
import asyncio
from artemis.rl_algorithms.grpo import GRPO, GRPOConfig
from artemis.agents.llm_agent import LLMAgent, LLMAgentConfig
from artemis.environments.math_env import MathematicalReasoningEnvironment

async def grpo_training_example():
    # Configure GRPO algorithm
    grpo_config = GRPOConfig(
        num_reference_policies=3,
        reference_weight=0.1,
        learning_rate=1e-4,
        batch_size=32
    )
    
    # Configure agent with GRPO
    agent_config = LLMAgentConfig(
        model_name="WizardLM/WizardMath-7B-V1.1",
        use_grpo=True,
        grpo_beta=0.1
    )
    
    agent = LLMAgent(agent_config)
    environment = MathematicalReasoningEnvironment()
    grpo = GRPO(grpo_config)
    
    # Training loop
    experience_buffer = []
    
    for episode in range(1000):
        observation = environment.reset()
        episode_experience = []
        
        while not environment.done:
            action = agent.get_action(observation)
            step_result = environment.step(action)
            
            experience = {
                'observation': observation,
                'action': action,
                'reward': step_result.reward,
                'next_observation': step_result.observation,
                'done': step_result.done
            }
            
            episode_experience.append(experience)
            observation = step_result.observation
        
        experience_buffer.extend(episode_experience)
        
        # Update policy every 32 episodes
        if len(experience_buffer) >= grpo_config.batch_size:
            metrics = grpo.update(experience_buffer)
            experience_buffer = []
            
            print(f"Episode {episode}")
            print(f"Policy loss: {metrics['policy_loss']:.4f}")
            print(f"Value loss: {metrics['value_loss']:.4f}")
            print(f"Group KL divergence: {metrics['group_kl_div']:.4f}")

asyncio.run(grpo_training_example())
```

## Direct Preference Optimization (DPO)

DPO eliminates the need for a separate reward model by directly optimizing on preference data:

### Core Concept

DPO reparameterizes the reward function in terms of the optimal policy:

```python
# DPO loss function
L_DPO = -E[(log œÉ(Œ≤ log(œÄ_Œ∏(y_w|x)/œÄ_ref(y_w|x)) - Œ≤ log(œÄ_Œ∏(y_l|x)/œÄ_ref(y_l|x))))]
```

Where:
- `y_w` is the preferred response
- `y_l` is the less preferred response  
- `Œ≤` controls the deviation from the reference policy
- `œÉ` is the sigmoid function

### Configuration

```python
from artemis.rl_algorithms.dpo import DPO, DPOConfig

config = DPOConfig(
    beta=0.1,                    # KL regularization strength
    reference_free=False,        # Whether to use reference-free DPO
    label_smoothing=0.0,         # Label smoothing for preferences
    loss_type="sigmoid",         # "sigmoid", "hinge", "ipo"
    preference_data_ratio=1.0,   # Ratio of preference to standard data
    
    # Training parameters
    learning_rate=5e-5,
    batch_size=32,
    max_grad_norm=1.0,
    warmup_steps=100
)

dpo = DPO(config)
```

### Key Features

<Tabs>
<Tab title="Direct Preference Learning">
  ```python
  class DPOLoss:
      def __init__(self, beta: float, loss_type: str = "sigmoid"):
          self.beta = beta
          self.loss_type = loss_type
          
      def compute_loss(self, policy_preferred_logps, policy_dispreferred_logps,
                      reference_preferred_logps, reference_dispreferred_logps):
          
          # Compute log ratios
          preferred_ratio = policy_preferred_logps - reference_preferred_logps
          dispreferred_ratio = policy_dispreferred_logps - reference_dispreferred_logps
          
          # DPO objective
          logits = self.beta * (preferred_ratio - dispreferred_ratio)
          
          if self.loss_type == "sigmoid":
              return -torch.nn.functional.logsigmoid(logits).mean()
          elif self.loss_type == "hinge":
              return torch.nn.functional.relu(1 - logits).mean()
          elif self.loss_type == "ipo":
              return (logits - 1/(2*self.beta))**2
  ```
</Tab>

<Tab title="Preference Data Handling">
  ```python
  @dataclass
  class PreferenceExample:
      prompt: str
      preferred_response: str
      dispreferred_response: str
      preference_strength: float = 1.0
      metadata: Dict[str, Any] = None

  class PreferenceDataLoader:
      def __init__(self, preference_data, batch_size: int):
          self.data = preference_data
          self.batch_size = batch_size
          
      def get_batch(self):
          batch = random.sample(self.data, self.batch_size)
          return {
              'prompts': [ex.prompt for ex in batch],
              'preferred': [ex.preferred_response for ex in batch],
              'dispreferred': [ex.dispreferred_response for ex in batch]
          }
  ```
</Tab>

<Tab title="Reference Model Integration">
  ```python
  class DPOWithReference:
      def __init__(self, policy_model, reference_model, config):
          self.policy = policy_model
          self.reference = reference_model
          self.config = config
          
      def compute_log_probs(self, model, sequences, prompts):
          with torch.no_grad() if model == self.reference else nullcontext():
              outputs = model(sequences, labels=sequences)
              log_probs = torch.nn.functional.log_softmax(outputs.logits, dim=-1)
              
              # Extract log probabilities for the actual tokens
              return log_probs.gather(-1, sequences.unsqueeze(-1)).squeeze(-1)
  ```
</Tab>
</Tabs>

### Usage Example

```python
from artemis.rl_algorithms.dpo import DPO, DPOConfig
from artemis.data.preference_data import PreferenceDataset

async def dpo_training_example():
    # Load preference data
    preference_dataset = PreferenceDataset.from_file("human_preferences.json")
    
    # Configure DPO
    dpo_config = DPOConfig(
        beta=0.1,
        loss_type="sigmoid",
        learning_rate=5e-5,
        batch_size=16
    )
    
    # Agent configuration for DPO
    agent_config = LLMAgentConfig(
        model_name="meta-llama/Llama-3.1-8B-Instruct",
        use_dpo=True,
        dpo_beta=0.1
    )
    
    agent = LLMAgent(agent_config)
    dpo = DPO(dpo_config)
    
    # Training loop
    for epoch in range(10):
        total_loss = 0
        num_batches = 0
        
        for batch in preference_dataset.get_batches(dpo_config.batch_size):
            # Forward pass
            policy_preferred_logps = agent.get_log_probs(
                batch['prompts'], batch['preferred_responses']
            )
            policy_dispreferred_logps = agent.get_log_probs(
                batch['prompts'], batch['dispreferred_responses']
            )
            
            # Reference model log probs (cached or computed)
            ref_preferred_logps = dpo.get_reference_log_probs(
                batch['prompts'], batch['preferred_responses']
            )
            ref_dispreferred_logps = dpo.get_reference_log_probs(
                batch['prompts'], batch['dispreferred_responses']
            )
            
            # Compute DPO loss
            loss = dpo.compute_loss(
                policy_preferred_logps, policy_dispreferred_logps,
                ref_preferred_logps, ref_dispreferred_logps
            )
            
            # Update
            loss.backward()
            dpo.step()
            
            total_loss += loss.item()
            num_batches += 1
        
        avg_loss = total_loss / num_batches
        print(f"Epoch {epoch}: Average DPO loss = {avg_loss:.4f}")

asyncio.run(dpo_training_example())
```

## Proximal Policy Optimization (PPO)

PPO serves as a reliable baseline algorithm with proven stability:

### Configuration

```python
from artemis.rl_algorithms.ppo import PPO, PPOConfig

config = PPOConfig(
    learning_rate=1e-4,
    clip_epsilon=0.2,           # PPO clipping parameter
    value_coefficient=0.5,      # Value function loss coefficient
    entropy_coefficient=0.01,   # Entropy bonus coefficient
    max_grad_norm=1.0,          # Gradient clipping
    batch_size=64,
    epochs_per_update=4,
    use_gae=True,              # Generalized Advantage Estimation
    gae_lambda=0.95,           # GAE lambda parameter
    normalize_advantages=True   # Advantage normalization
)

ppo = PPO(config)
```

### Usage Example

```python
from artemis.rl_algorithms.ppo import PPO, PPOConfig

async def ppo_training_example():
    # Standard PPO configuration
    ppo_config = PPOConfig(
        learning_rate=1e-4,
        clip_epsilon=0.2,
        batch_size=64
    )
    
    agent_config = LLMAgentConfig(
        model_name="microsoft/DialoGPT-medium",
        use_ppo=True
    )
    
    agent = LLMAgent(agent_config)
    environment = ConversationEnvironment()
    ppo = PPO(ppo_config)
    
    # Training follows standard PPO procedure
    # (Similar to GRPO example but with PPO-specific updates)

asyncio.run(ppo_training_example())
```

## Algorithm Selection Guide

<Tabs>
<Tab title="Use PRISM When">
  - Complex multi-step planning is required
  - Working with sparse or delayed rewards
  - Risk-sensitive decision making is important
  - Sample efficiency is critical
  - You need uncertainty quantification
  - Working with dynamic/uncertain environments

  **Example scenarios:**
  - Strategic game playing
  - Financial decision making
  - Autonomous systems
  - Medical diagnosis/treatment
  - Scientific discovery tasks
</Tab>

<Tab title="Use GRPO When">
  - Training large language models (>1B parameters)
  - Sample efficiency is critical
  - You have computational resources for multiple reference policies
  - Training stability is important
  - You want state-of-the-art performance

  **Example scenarios:**
  - Production LLM fine-tuning
  - Large-scale research experiments
  - Complex multi-task learning
</Tab>

<Tab title="Use DPO When">
  - You have preference data from humans
  - Safety and alignment are critical
  - You want to avoid reward model training
  - Working with instruction-following tasks
  - Human feedback is available

  **Example scenarios:**
  - Chatbot alignment
  - Content safety training
  - Instruction following
  - Human preference learning
</Tab>

<Tab title="Use PPO When">
  - Starting with RL for LLMs
  - Need a well-understood baseline
  - Limited computational resources
  - Comparing against literature
  - Prototyping new environments

  **Example scenarios:**
  - Research baselines
  - Initial experiments
  - Educational purposes
  - Resource-constrained settings
</Tab>
</Tabs>

## Next Steps

<CardGroup cols={2}>
<Card title="Distributed Training" icon="network-wired" href="/artemis/distributed/overview">
  Learn how to scale these algorithms across multiple nodes
</Card>

<Card title="Examples" icon="code" href="/artemis/examples">
  See practical implementations of each algorithm
</Card>
</CardGroup>