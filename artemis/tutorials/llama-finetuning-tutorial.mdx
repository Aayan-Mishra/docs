---
title: "Fine-tuning Llama 8B with Artemis and Claude Sonnet 4 Teacher"
description: "Complete guide to fine-tuning Meta Llama 8B Instruct using Artemis RL Gym with Claude Sonnet 4 as a teacher agent"
---

This tutorial demonstrates how to fine-tune Meta Llama 8B Instruct using **Artemis RL Gym's built-in trainers** with Claude Sonnet 4 as a teacher agent for evaluation and guidance. We'll use Artemis's PPO trainer with integrated Claude teacher evaluation, then deploy the fine-tuned model to Hugging Face in SafeTensors format.

## Prerequisites

<CardGroup cols={2}>
<Card title="Hardware Requirements" icon="microchip">
  - GPU with 24GB+ VRAM (RTX 4090, A100, H100)
  - 32GB+ System RAM
  - 100GB+ Storage space
</Card>
<Card title="Software Requirements" icon="code">
  - Python 3.9+
  - CUDA 11.8+
  - Git LFS
  - Hugging Face account
  - **Artemis RL Gym**
</Card>
<Card title="API Access" icon="key">
  - Anthropic API key for Claude Sonnet 4
  - Hugging Face write token
  - Meta Llama access approval
</Card>
<Card title="Knowledge Level" icon="graduation-cap">
  - Intermediate Python
  - Basic RL concepts
  - Command line proficiency
</Card>
</CardGroup>

## Environment Setup

### 1. Install Artemis RL Gym

```bash
# Create virtual environment
python -m venv artemis_llama
source artemis_llama/bin/activate  # Linux/Mac
# artemis_llama\Scripts\activate  # Windows

# Install Artemis RL Gym
pip install git+https://github.com/Noema-Research/ArtemisRL-Gym.git

# Install additional dependencies for teacher agent
pip install anthropic
pip install wandb  # Optional for enhanced logging
```

### 2. Environment Variables

Create a `.env` file in your project directory:

```bash
# .env file
ANTHROPIC_API_KEY=your_anthropic_api_key_here
HF_TOKEN=your_huggingface_token_here
WANDB_API_KEY=your_wandb_key_here  # Optional
```

### 3. Hugging Face Authentication

```bash
# Login to Hugging Face
huggingface-cli login --token $HF_TOKEN

# Accept Meta Llama license (required)
# Visit: https://huggingface.co/meta-llama/Llama-2-8b-chat-hf
```

## Project Structure

```
artemis-llama-finetuning/
‚îú‚îÄ‚îÄ .env
‚îú‚îÄ‚îÄ main.py
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ claude_teacher.py
‚îÇ   ‚îú‚îÄ‚îÄ artemis_config.py
‚îÇ   ‚îî‚îÄ‚îÄ model_upload.py
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îî‚îÄ‚îÄ prompts.json
‚îú‚îÄ‚îÄ models/
‚îÇ   ‚îî‚îÄ‚îÄ llama-8b-artemis/
‚îî‚îÄ‚îÄ configs/
    ‚îî‚îÄ‚îÄ training_config.yaml
```

## Step 1: Claude Teacher Agent with Artemis Integration

<Tabs>
<Tab title="Claude Teacher Agent">
```python
# src/claude_teacher.py
import os
import asyncio
from typing import Dict, List, Any
from anthropic import Anthropic
from dotenv import load_dotenv
from artemis.agent import agent
from artemis.core.agent import BaseAgent, AgentConfig

load_dotenv()

class ClaudeTeacherAgent(BaseAgent):
    """Claude Sonnet 4 teacher agent integrated with Artemis framework"""
    
    def __init__(self, config: AgentConfig = None):
        super().__init__(config or AgentConfig())
        self.client = Anthropic(api_key=os.getenv("ANTHROPIC_API_KEY"))
        self.model = "claude-3-5-sonnet-20250514"
    
    async def evaluate_response(self, instruction: str, student_response: str) -> Dict[str, Any]:
        """Evaluate student model response using Claude as teacher"""
        
        system_prompt = """You are an expert AI teacher evaluating responses from a student language model being trained with reinforcement learning.
        
        Provide a reward score between -1.0 and 1.0:
        - 1.0: Excellent response, exceeds expectations
        - 0.5-0.9: Good response with minor issues  
        - 0.0-0.4: Adequate response but needs improvement
        - -0.1 to -0.5: Poor response with significant issues
        - -0.6 to -1.0: Harmful, incorrect, or inappropriate response
        
        Return JSON format: {"reward": float, "reasoning": str}"""
        
        user_prompt = f"""
Instruction: {instruction}

Student Response: {student_response}

Evaluate and provide reward score with reasoning."""
        
        try:
            response = await self.client.messages.create(
                model=self.model,
                max_tokens=512,
                temperature=0.2,  # Lower temperature for consistent evaluation
                system=system_prompt,
                messages=[{"role": "user", "content": user_prompt}]
            )
            
            content = response.content[0].text
            
            # Parse the JSON response (simplified - in production use proper JSON parsing)
            import re
            import json
            
            try:
                # Try to extract JSON from response
                json_match = re.search(r'\{.*\}', content, re.DOTALL)
                if json_match:
                    result = json.loads(json_match.group())
                    reward = float(result.get("reward", 0.0))
                    reasoning = result.get("reasoning", content)
                else:
                    # Fallback to score extraction
                    score_match = re.search(r'[-]?\d+\.?\d*', content)
                    reward = float(score_match.group()) if score_match else 0.0
                    reasoning = content
            except:
                reward = 0.0
                reasoning = content
            
            # Clamp reward to valid range
            reward = max(-1.0, min(1.0, reward))
            
            return {
                "reward": reward,
                "reasoning": reasoning,
                "teacher_model": self.model,
                "evaluation_full": content
            }
            
        except Exception as e:
            print(f"Error evaluating response: {e}")
            return {"reward": 0.0, "reasoning": "Error occurred", "teacher_model": self.model}

# Create teacher agent using Artemis factory
def create_claude_teacher():
    """Create Claude teacher agent using Artemis agent factory"""
    
    # Configure Claude teacher agent
    claude_config = AgentConfig(
        agent_type="teacher",
        model_name="claude-3-5-sonnet-20250514",
        provider="anthropic"
    )
    
    # Create using Artemis agent factory
    teacher = agent().config(claude_config).create(ClaudeTeacherAgent)
    
    return teacher

# Example usage
async def test_teacher():
    teacher = create_claude_teacher()
    
    instruction = "Explain the concept of machine learning"
    response = "Machine learning is when computers learn patterns from data automatically."
    
    evaluation = await teacher.evaluate_response(instruction, response)
    print(f"Reward: {evaluation['reward']}")
    print(f"Reasoning: {evaluation['reasoning']}")

if __name__ == "__main__":
    asyncio.run(test_teacher())
```
</Tab>

<Tab title="Artemis Reward Model">
```python
# src/artemis_reward.py
import asyncio
from typing import List, Dict
from artemis.core.reward import BaseRewardModel, RewardConfig
from .claude_teacher import ClaudeTeacherAgent

class ClaudeRewardModel(BaseRewardModel):
    """Artemis-compatible reward model using Claude teacher"""
    
    def __init__(self, config: RewardConfig = None):
        super().__init__(config or RewardConfig())
        self.teacher_agent = ClaudeTeacherAgent()
    
    async def compute_reward(self, prompt: str, response: str) -> float:
        """Compute reward for a prompt-response pair"""
        evaluation = await self.teacher_agent.evaluate_response(prompt, response)
        return evaluation["reward"]
    
    async def compute_batch_rewards(self, prompts: List[str], responses: List[str]) -> List[float]:
        """Compute rewards for a batch of prompt-response pairs"""
        tasks = [
            self.compute_reward(prompt, response) 
            for prompt, response in zip(prompts, responses)
        ]
        
        # Add delay to respect API rate limits
        results = []
        for task in tasks:
            reward = await task
            results.append(reward)
            await asyncio.sleep(0.5)  # Rate limiting
        
        return results
    
    def get_reward_statistics(self) -> Dict:
        """Get reward model statistics"""
        return {
            "model_name": "claude-3-5-sonnet-20250514",
            "reward_range": [-1.0, 1.0],
            "evaluation_type": "teacher_guided"
        }
```
</Tab>
</Tabs>

## Step 2: Artemis Training Configuration

<Tabs>
<Tab title="PPO Configuration">
```python
# src/artemis_config.py
import os
from artemis.training import PPOConfig, PPOTrainer
from artemis.environments.dataset_envs.ultrachat import UltraChatEnvironment
from artemis.agents.llm_agent import LLMAgent, LLMAgentConfig
from .artemis_reward import ClaudeRewardModel

def create_ppo_config():
    """Create PPO configuration for Llama fine-tuning with Claude teacher"""
    
    config = PPOConfig(
        # Model settings
        model_name="meta-llama/Llama-2-8b-chat-hf",
        
        # PPO hyperparameters optimized for instruction following
        learning_rate=1.41e-5,
        batch_size=8,
        mini_batch_size=2,
        gradient_accumulation_steps=4,
        ppo_epochs=4,
        
        # PPO-specific parameters
        cliprange=0.2,           # Policy clipping range
        cliprange_value=0.2,     # Value function clipping
        vf_coef=0.1,            # Value function coefficient
        kl_coef=0.02,           # KL divergence coefficient  
        target_kl=0.01,         # Target KL divergence
        
        # Generation parameters for student model
        max_new_tokens=512,
        temperature=0.7,
        top_k=50,
        top_p=0.95,
        do_sample=True,
        
        # Training settings
        max_epochs=3,
        warmup_steps=100,
        max_steps=1000,
        
        # Optimization
        optimizer="adamw",
        weight_decay=0.01,
        max_grad_norm=1.0,
        scheduler="cosine",
        
        # Memory optimization for 24GB GPU
        gradient_checkpointing=True,
        fp16=True,
        dataloader_num_workers=4,
        
        # Logging and evaluation
        logging_steps=10,
        save_steps=200,
        eval_steps=100,
        output_dir="./models/llama-8b-artemis",
        report_to="wandb",
        run_name="llama-8b-claude-teacher-ppo",
        
        # Teacher-specific settings
        use_teacher_forcing=True,
        teacher_evaluation_frequency=50,  # Evaluate with teacher every 50 steps
        reward_model_type="claude_teacher"
    )
    
    return config

def create_llm_agent():
    """Create LLM agent for Llama 8B with LoRA"""
    
    agent_config = LLMAgentConfig(
        model_name="meta-llama/Llama-2-8b-chat-hf",
        
        # LoRA configuration for memory efficiency
        use_lora=True,
        lora_config={
            "r": 16,
            "lora_alpha": 32,
            "lora_dropout": 0.1,
            "target_modules": ["q_proj", "v_proj", "k_proj", "o_proj", 
                             "gate_proj", "up_proj", "down_proj"]
        },
        
        # Quantization for 24GB GPU
        load_in_4bit=True,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_compute_dtype="float16",
        bnb_4bit_use_double_quant=True,
        
        # Generation settings
        max_new_tokens=512,
        temperature=0.7,
        do_sample=True,
        pad_token_id=None  # Will be set automatically
    )
    
    return LLMAgent(agent_config)

def create_environment():
    """Create UltraChat environment for instruction following"""
    
    from artemis.environments.dataset_envs.ultrachat import UltraChatEnvironment
    
    env = UltraChatEnvironment(
        dataset_name="HuggingFaceH4/ultrachat_200k",
        split="train_sft",
        max_samples=1000,  # Subset for efficient training
        instruction_format="llama2",  # Use Llama 2 chat format
        max_length=2048
    )
    
    return env
```
</Tab>

<Tab title="Alternative: DPO Configuration">
```python
# Alternative: Use DPO instead of PPO
from artemis.training import DPOConfig, DPOTrainer

def create_dpo_config():
    """Create DPO configuration for preference learning with Claude teacher"""
    
    config = DPOConfig(
        # Model settings
        model_name="meta-llama/Llama-2-8b-chat-hf",
        
        # DPO hyperparameters
        beta=0.1,
        label_smoothing=0.0,
        
        # Training hyperparameters
        learning_rate=5e-7,
        batch_size=4,
        gradient_accumulation_steps=4,
        max_epochs=3,
        warmup_steps=100,
        
        # Optimization
        optimizer="adamw",
        scheduler="cosine", 
        weight_decay=0.01,
        max_grad_norm=1.0,
        
        # Memory optimization
        gradient_checkpointing=True,
        fp16=True,
        dataloader_num_workers=4,
        
        # Logging and saving
        logging_steps=10,
        save_steps=500,
        eval_steps=250,
        output_dir="./models/llama-8b-artemis-dpo",
        
        # Evaluation
        eval_strategy="steps",
        load_best_model_at_end=True,
        metric_for_best_model="eval_loss",
        
        # Wandb logging
        report_to="wandb",
        run_name="llama-8b-claude-teacher-dpo"
    )
    
    return config
```
</Tab>

<Tab title="GRPO Configuration">
```python
# Alternative: Use GRPO (Group Relative Policy Optimization)
from artemis.rl_algorithms.grpo import GRPOConfig, GRPO

def create_grpo_config():
    """Create GRPO configuration - Artemis's advanced RL algorithm"""
    
    config = GRPOConfig(
        # Model settings
        model_name="meta-llama/Llama-2-8b-chat-hf",
        
        # GRPO-specific parameters
        num_reference_policies=3,
        reference_weight=0.1,
        group_size=4,
        relative_threshold=0.1,
        
        # RL hyperparameters  
        learning_rate=1e-4,
        kl_penalty_coefficient=0.02,
        clip_epsilon=0.2,
        value_function_coeff=0.1,
        
        # Training settings
        batch_size=8,
        mini_batch_size=2,
        gradient_accumulation_steps=4,
        max_epochs=3,
        max_steps=1000,
        
        # Generation parameters
        max_new_tokens=512,
        temperature=0.7,
        top_p=0.95,
        
        # Memory optimization
        gradient_checkpointing=True,
        fp16=True,
        
        # Logging
        output_dir="./models/llama-8b-artemis-grpo",
        logging_steps=10,
        save_steps=200,
        eval_steps=100
    )
    
    return config
```
</Tab>
</Tabs>

## Step 3: Artemis PPO Training with Claude Teacher

<Tabs>
<Tab title="Main Training Script">
```python
# main.py - Complete Artemis training workflow
import asyncio
import os
from dotenv import load_dotenv

# Artemis imports
from artemis.training import PPOTrainer, PPOConfig
from artemis.agents.llm_agent import LLMAgent
from artemis.environments.dataset_envs.ultrachat import UltraChatEnvironment
from artemis import train_llm_grpo
from artemis.rl_algorithms.grpo import GRPOConfig

# Custom components
from src.artemis_config import create_ppo_config, create_llm_agent, create_environment
from src.artemis_reward import ClaudeRewardModel
from src.claude_teacher import create_claude_teacher
from src.model_upload import ArtemisModelUploader

async def main():
    """Complete Artemis fine-tuning workflow with Claude teacher"""
    
    load_dotenv()
    
    print("üöÄ Starting Llama 8B Fine-tuning with Artemis RL Gym + Claude Teacher")
    
    # Step 1: Initialize Artemis components
    print("\n‚öôÔ∏è Step 1: Initializing Artemis components...")
    
    # Create PPO configuration
    config = create_ppo_config()
    
    # Create LLM agent with LoRA
    llm_agent = create_llm_agent()
    print(f"‚úÖ LLM Agent initialized: {llm_agent.model_name}")
    
    # Create environment with UltraChat dataset
    environment = create_environment()
    print(f"‚úÖ Environment created with {len(environment.dataset)} samples")
    
    # Create Claude teacher reward model
    reward_model = ClaudeRewardModel()
    teacher_agent = create_claude_teacher()
    print("‚úÖ Claude teacher agent initialized")
    
    # Step 2: Initialize Artemis PPO Trainer
    print("\nüßë‚Äçüè´ Step 2: Setting up Artemis PPO Trainer...")
    
    trainer = PPOTrainer(
        model=llm_agent,
        config=config,
        environment=environment,
        reward_model=reward_model,
        teacher_agent=teacher_agent  # Artemis will handle teacher integration
    )
    
    print("‚úÖ PPO Trainer initialized with Claude teacher integration")
    
    # Step 3: Start training with Artemis
    print("\nüî• Step 3: Starting Artemis PPO training with teacher guidance...")
    
    # Artemis handles the entire training loop including:
    # - Policy updates with PPO
    # - Teacher evaluation at specified intervals  
    # - Reward model integration
    # - Memory management and optimization
    # - Distributed training (if configured)
    # - Real-time monitoring
    
    training_results = await trainer.train()
    
    print("‚úÖ Artemis PPO training completed!")
    print(f"üìä Training Results:")
    print(f"  Final Reward: {training_results.final_reward:.3f}")
    print(f"  Training Episodes: {training_results.total_episodes}")
    print(f"  Total Steps: {training_results.total_steps}")
    print(f"  Training Time: {training_results.training_time:.2f}s")
    
    # Step 4: Model evaluation with Artemis
    print("\nüìà Step 4: Evaluating trained model...")
    
    evaluation_results = await trainer.evaluate(
        eval_episodes=100,
        use_teacher_evaluation=True  # Use Claude teacher for evaluation
    )
    
    print(f"üìä Evaluation Results:")
    print(f"  Average Reward: {evaluation_results['avg_reward']:.3f}")
    print(f"  Teacher Approval Rate: {evaluation_results['teacher_approval_rate']:.1%}")
    print(f"  Policy Improvement: {evaluation_results['improvement_over_base']:.3f}")
    
    # Step 5: Save and upload model
    print("\nüíæ Step 5: Saving and uploading trained model...")
    
    # Save using Artemis built-in model saving
    model_path = trainer.save_model("./models/llama-8b-artemis-ppo")
    
    # Upload to Hugging Face with Artemis integration
    uploader = ArtemisModelUploader(
        trainer=trainer,
        hf_token=os.getenv("HF_TOKEN")
    )
    
    repo_url = await uploader.upload_to_hf(
        repo_name="your-username/llama-8b-artemis-claude-teacher",
        private=False,
        include_training_metadata=True  # Include Artemis training logs
    )
    
    print(f"üéâ Model uploaded to: {repo_url}")
    
    return {
        'training_results': training_results,
        'evaluation_results': evaluation_results,
        'model_path': model_path,
        'repo_url': repo_url
    }

# Alternative: Use high-level Artemis function
async def artemis_high_level_training():
    """Use Artemis high-level training function"""
    
    # Configure GRPO (Artemis's advanced RL algorithm)
    grpo_config = GRPOConfig(
        learning_rate=1e-4,
        num_reference_policies=3,
        reference_weight=0.1,
        kl_penalty_coefficient=0.02,
        clip_epsilon=0.2
    )
    
    # Train using Artemis high-level API
    results = await train_llm_grpo(
        model_name="meta-llama/Llama-2-8b-chat-hf",
        env_type="ultrachat",  # Built-in UltraChat environment
        num_episodes=500,
        algorithm_config=grpo_config,
        reward_model="claude_teacher",  # Use Claude teacher as reward model
        distributed=True,  # Enable distributed training
        monitoring=True,   # Enable real-time monitoring
        output_dir="./models/llama-8b-artemis-grpo",
        teacher_config={
            "model": "claude-3-5-sonnet-20250514",
            "evaluation_frequency": 50,
            "reward_shaping": True
        }
    )
    
    print(f"üéâ High-level Artemis training completed!")
    print(f"Final reward: {results.final_reward:.3f}")
    
    return results

if __name__ == "__main__":
    # Choose training method
    use_high_level_api = True
    
    if use_high_level_api:
        results = asyncio.run(artemis_high_level_training())
    else:
        results = asyncio.run(main())
```
</Tab>

<Tab title="Artemis Training Monitor">
```python
# src/artemis_monitor.py
import wandb
from artemis.utils.monitoring import TrainingMonitor, MetricsLogger
from artemis.utils.callbacks import ArtemisCallback

class ClaudeTeacherMonitor(ArtemisCallback):
    """Custom Artemis callback for monitoring Claude teacher evaluations"""
    
    def __init__(self, teacher_agent, log_frequency=50):
        super().__init__()
        self.teacher_agent = teacher_agent
        self.log_frequency = log_frequency
        self.teacher_evaluations = []
    
    async def on_episode_end(self, episode_data):
        """Called at the end of each training episode"""
        
        if episode_data['episode'] % self.log_frequency == 0:
            # Get teacher evaluation
            prompt = episode_data['prompt']
            response = episode_data['response']
            
            teacher_eval = await self.teacher_agent.evaluate_response(prompt, response)
            
            # Log to Artemis monitoring system
            self.log_metrics({
                'teacher_reward': teacher_eval['reward'],
                'teacher_reasoning_length': len(teacher_eval['reasoning']),
                'episode': episode_data['episode']
            })
            
            # Store for analysis
            self.teacher_evaluations.append({
                'episode': episode_data['episode'],
                'teacher_reward': teacher_eval['reward'],
                'ppo_reward': episode_data['reward'],
                'prompt': prompt,
                'response': response,
                'teacher_reasoning': teacher_eval['reasoning']
            })
    
    def get_teacher_statistics(self):
        """Get statistics about teacher evaluations"""
        if not self.teacher_evaluations:
            return {}
        
        rewards = [e['teacher_reward'] for e in self.teacher_evaluations]
        
        return {
            'avg_teacher_reward': sum(rewards) / len(rewards),
            'max_teacher_reward': max(rewards),
            'min_teacher_reward': min(rewards),
            'total_evaluations': len(self.teacher_evaluations),
            'positive_rate': sum(1 for r in rewards if r > 0) / len(rewards)
        }

def setup_artemis_monitoring(trainer, teacher_agent):
    """Setup comprehensive monitoring for Artemis training"""
    
    # Initialize Weights & Biases with Artemis integration
    wandb.init(
        project="artemis-llama-claude-teacher",
        name="llama-8b-ppo-claude-guidance",
        config=trainer.config.__dict__,
        tags=["artemis", "ppo", "claude-teacher", "llama-8b"]
    )
    
    # Add Artemis training monitor
    training_monitor = TrainingMonitor(
        log_frequency=10,
        save_frequency=100,
        metrics_to_track=[
            'reward/mean',
            'reward/std', 
            'policy_loss',
            'value_loss',
            'kl_divergence',
            'teacher_reward',
            'learning_rate'
        ]
    )
    
    # Add Claude teacher monitor
    teacher_monitor = ClaudeTeacherMonitor(teacher_agent, log_frequency=50)
    
    # Register callbacks with trainer
    trainer.add_callback(training_monitor)
    trainer.add_callback(teacher_monitor)
    
    return training_monitor, teacher_monitor
```
</Tab>

<Tab title="Distributed Training">
```python
# src/distributed_training.py
from artemis.distributed import DeploymentManager, DistributedConfig
from artemis.rl_algorithms.distributed import DistributedPPO

async def setup_distributed_training():
    """Setup distributed training with Artemis"""
    
    # Configure distributed training
    distributed_config = DistributedConfig(
        num_workers=4,
        num_environments_per_worker=2,
        communication_backend="nccl",
        gradient_compression=True,
        async_updates=True
    )
    
    # Deploy distributed training
    deployment_manager = DeploymentManager()
    
    # Setup distributed cluster
    cluster = await deployment_manager.create_cluster(
        config=distributed_config,
        auto_scale=True,
        fault_tolerance=True
    )
    
    # Create distributed PPO trainer
    distributed_trainer = DistributedPPO(
        config=create_ppo_config(),
        cluster=cluster,
        load_balancing=True
    )
    
    print(f"üåê Distributed training setup with {distributed_config.num_workers} workers")
    
    return distributed_trainer

# Example distributed training
async def distributed_training_example():
    """Example of distributed training with Artemis"""
    
    trainer = await setup_distributed_training()
    
    # Train with distributed setup
    results = await trainer.train(
        total_episodes=2000,
        checkpoint_frequency=200,
        fault_recovery=True
    )
    
    print(f"Distributed training completed: {results.final_reward:.3f}")
    
    return results
```
</Tab>
</Tabs>

## Step 4: Model Evaluation and Deployment

<Tabs>
<Tab title="Artemis Model Evaluation">
```python
# src/artemis_evaluation.py
import asyncio
from typing import Dict, List
from artemis.evaluation import ModelEvaluator, EvaluationConfig
from artemis.utils.metrics import RewardMetrics, PolicyMetrics
from .claude_teacher import create_claude_teacher

class ArtemisLlamaEvaluator:
    """Comprehensive evaluation using Artemis framework with Claude teacher"""
    
    def __init__(self, trained_model_path: str):
        self.model_path = trained_model_path
        self.teacher_agent = create_claude_teacher()
        
        # Initialize Artemis evaluator
        eval_config = EvaluationConfig(
            model_path=trained_model_path,
            evaluation_type="instruction_following",
            metrics=["reward", "policy_improvement", "teacher_agreement"],
            batch_size=8,
            use_teacher_evaluation=True
        )
        
        self.evaluator = ModelEvaluator(eval_config)
    
    async def comprehensive_evaluation(self, test_prompts: List[str]) -> Dict:
        """Run comprehensive evaluation with Artemis + Claude teacher"""
        
        print("üß™ Starting comprehensive evaluation with Artemis...")
        
        # Artemis built-in evaluation
        artemis_results = await self.evaluator.evaluate(
            test_data=test_prompts,
            metrics=["policy_performance", "generation_quality", "safety"]
        )
        
        # Claude teacher evaluation
        teacher_results = await self._teacher_evaluation(test_prompts)
        
        # Policy improvement analysis
        improvement_analysis = await self._analyze_policy_improvement(test_prompts)
        
        # Combine results
        evaluation_summary = {
            'artemis_metrics': artemis_results,
            'teacher_evaluation': teacher_results,
            'policy_improvement': improvement_analysis,
            'overall_score': self._calculate_overall_score(artemis_results, teacher_results)
        }
        
        return evaluation_summary
    
    async def _teacher_evaluation(self, test_prompts: List[str]) -> Dict:
        """Detailed teacher evaluation"""
        
        results = []
        total_reward = 0
        
        for prompt in test_prompts:
            # Generate response with trained model
            response = await self.evaluator.generate_response(prompt)
            
            # Get teacher evaluation
            teacher_eval = await self.teacher_agent.evaluate_response(prompt, response)
            
            results.append({
                'prompt': prompt,
                'response': response,
                'teacher_reward': teacher_eval['reward'],
                'teacher_reasoning': teacher_eval['reasoning']
            })
            
            total_reward += teacher_eval['reward']
            
            # Rate limiting
            await asyncio.sleep(0.5)
        
        return {
            'average_teacher_reward': total_reward / len(results),
            'detailed_evaluations': results,
            'positive_rate': sum(1 for r in results if r['teacher_reward'] > 0) / len(results),
            'excellent_rate': sum(1 for r in results if r['teacher_reward'] > 0.7) / len(results)
        }
    
    async def _analyze_policy_improvement(self, test_prompts: List[str]) -> Dict:
        """Analyze policy improvement over base model"""
        
        # Compare with base model using Artemis
        base_model_results = await self.evaluator.compare_with_baseline(
            baseline_model="meta-llama/Llama-2-8b-chat-hf",
            test_data=test_prompts
        )
        
        improvement_metrics = {
            'reward_improvement': base_model_results['reward_delta'],
            'quality_improvement': base_model_results['quality_delta'],
            'safety_improvement': base_model_results['safety_delta'],
            'improvement_rate': base_model_results['improvement_percentage']
        }
        
        return improvement_metrics
    
    def _calculate_overall_score(self, artemis_results: Dict, teacher_results: Dict) -> float:
        """Calculate overall evaluation score"""
        
        artemis_score = artemis_results.get('overall_score', 0.0)
        teacher_score = teacher_results.get('average_teacher_reward', 0.0)
        
        # Weighted combination (Artemis 60%, Teacher 40%)
        overall_score = (artemis_score * 0.6) + (teacher_score * 0.4)
        
        return overall_score

# Example evaluation
async def evaluate_trained_model():
    """Example evaluation of trained model"""
    
    evaluator = ArtemisLlamaEvaluator("./models/llama-8b-artemis-ppo")
    
    test_prompts = [
        "Explain machine learning algorithms with practical examples",
        "How do you optimize database performance for large datasets?",
        "What are the best practices for secure API design?",
        "Describe the benefits of microservices architecture",
        "How to implement effective error handling in production code?"
    ]
    
    results = await evaluator.comprehensive_evaluation(test_prompts)
    
    print(f"üìä Evaluation Results:")
    print(f"  Overall Score: {results['overall_score']:.3f}")
    print(f"  Teacher Reward: {results['teacher_evaluation']['average_teacher_reward']:.3f}")
    print(f"  Policy Improvement: {results['policy_improvement']['improvement_rate']:.1%}")
    print(f"  Positive Rate: {results['teacher_evaluation']['positive_rate']:.1%}")
    
    return results

if __name__ == "__main__":
    results = asyncio.run(evaluate_trained_model())
```
</Tab>

<Tab title="Model Upload with Artemis">
```python
# src/model_upload.py
import os
from artemis.utils.model_export import ModelExporter, ExportConfig
from artemis.utils.hf_integration import HuggingFaceUploader
from huggingface_hub import HfApi, create_repo

class ArtemisModelUploader:
    """Upload Artemis-trained model to Hugging Face with metadata"""
    
    def __init__(self, trainer, hf_token: str):
        self.trainer = trainer
        self.hf_token = hf_token
        self.api = HfApi(token=hf_token)
        
        # Initialize Artemis model exporter
        export_config = ExportConfig(
            format="safetensors",
            include_training_metadata=True,
            include_config=True,
            optimize_for_inference=True
        )
        
        self.exporter = ModelExporter(export_config)
        self.hf_uploader = HuggingFaceUploader(hf_token)
    
    async def upload_to_hf(self, repo_name: str, private: bool = False, 
                          include_training_metadata: bool = True) -> str:
        """Upload trained model to Hugging Face with Artemis integration"""
        
        print("üì§ Preparing model for upload with Artemis...")
        
        # Export model using Artemis
        export_path = await self.exporter.export_model(
            trainer=self.trainer,
            output_path="./export/llama-8b-artemis",
            merge_adapters=True,
            include_tokenizer=True
        )
        
        # Create model card with Artemis training metadata
        model_card = self._create_artemis_model_card(include_training_metadata)
        
        # Save model card
        with open(os.path.join(export_path, "README.md"), "w") as f:
            f.write(model_card)
        
        # Upload using Artemis HF integration
        repo_url = await self.hf_uploader.upload_model(
            model_path=export_path,
            repo_name=repo_name,
            private=private,
            commit_message="Upload Artemis-trained Llama 8B with Claude teacher guidance"
        )
        
        print(f"‚úÖ Model uploaded successfully to: {repo_url}")
        
        return repo_url
    
    def _create_artemis_model_card(self, include_training_metadata: bool) -> str:
        """Create comprehensive model card with Artemis metadata"""
        
        # Get training metadata from Artemis
        training_metadata = self.trainer.get_training_metadata()
        
        model_card = f"""---
language: en
license: llama2
library_name: transformers
tags:
- artemis-rl-gym
- ppo
- llama
- fine-tuned
- instruction-following
- claude-teacher-guided
- safetensors
- reinforcement-learning
base_model: meta-llama/Llama-2-8b-chat-hf
---

# Llama 8B Fine-tuned with Artemis RL Gym + Claude Sonnet 4 Teacher

This model is a fine-tuned version of [meta-llama/Llama-2-8b-chat-hf](https://huggingface.co/meta-llama/Llama-2-8b-chat-hf) using **Artemis RL Gym** with Claude Sonnet 4 (claude-3-5-sonnet-20250514) as a teacher agent for guidance and evaluation.

## Model Description

- **Base Model**: meta-llama/Llama-2-8b-chat-hf
- **Fine-tuning Framework**: Artemis RL Gym
- **Algorithm**: PPO (Proximal Policy Optimization)
- **Teacher Agent**: Claude Sonnet 4 (claude-3-5-sonnet-20250514)
- **Training Data**: UltraChat 200k dataset
- **Format**: SafeTensors

## Artemis Training Results

### Performance Metrics (Artemis + Claude Teacher)
- **Final Reward**: {training_metadata.get('final_reward', 'N/A'):.3f}
- **Policy Improvement**: {training_metadata.get('policy_improvement', 'N/A'):.3f}
- **Teacher Approval Rate**: {training_metadata.get('teacher_approval_rate', 'N/A'):.1%}
- **Training Episodes**: {training_metadata.get('total_episodes', 'N/A')}

### Training Configuration
- **Algorithm**: PPO with teacher guidance
- **Learning Rate**: {training_metadata.get('learning_rate', '1.41e-5')}
- **Batch Size**: {training_metadata.get('batch_size', '8')}
- **KL Coefficient**: {training_metadata.get('kl_coef', '0.02')}
- **Clip Range**: {training_metadata.get('cliprange', '0.2')}

## Usage with Artemis

```python
from artemis.agents.llm_agent import LLMAgent, LLMAgentConfig

# Load the trained model with Artemis
agent_config = LLMAgentConfig(
    model_name="your-username/llama-8b-artemis-claude-teacher",
    load_in_4bit=True,
    temperature=0.7
)

agent = LLMAgent(agent_config)

# Generate response
response = await agent.generate("Explain machine learning algorithms")
print(response)
```

## Standard Usage (without Artemis)

```python
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch

# Load model and tokenizer
model_name = "your-username/llama-8b-artemis-claude-teacher"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype=torch.float16,
    device_map="auto"
)

# Generate response
def generate_response(instruction):
    prompt = f'''### Instruction:
{{instruction}}

### Response:
'''
    
    inputs = tokenizer(prompt, return_tensors="pt")
    outputs = model.generate(**inputs, max_new_tokens=512, temperature=0.7)
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    
    return response.split("### Response:")[-1].strip()

# Example usage
response = generate_response("Explain machine learning algorithms")
print(response)
```

## Performance Highlights

This model demonstrates significant improvements over the base model through Artemis RL training:

1. **Enhanced Instruction Following**: Trained with PPO for better task completion
2. **Teacher-Validated Quality**: All responses evaluated by Claude Sonnet 4 during training
3. **Optimized Policy**: Reinforcement learning optimization for human preferences
4. **Consistent Performance**: Stable and reliable response quality across domains

## Training Process

The model was trained using the Artemis RL Gym framework, which provides:

- **Advanced RL Algorithms**: PPO implementation optimized for LLMs
- **Teacher Integration**: Seamless Claude Sonnet 4 evaluation during training
- **Memory Optimization**: Efficient training on 24GB GPUs with LoRA and quantization
- **Real-time Monitoring**: Comprehensive logging and evaluation metrics
- **Distributed Support**: Scalable training across multiple GPUs/nodes

## Limitations

- Inherits limitations from the base Llama 2 8B model
- Teacher evaluation based on Claude Sonnet 4's capabilities and potential biases
- Performance depends on the quality of teacher feedback during training

## Ethical Considerations

This model was trained using AI teacher guidance from Claude Sonnet 4 within the Artemis RL Gym framework. Users should be aware of potential biases from the base model, training data, and teacher model evaluations.

## Citation

```bibtex
@misc{{llama-8b-artemis-claude-teacher,
  title={{Llama 8B Fine-tuned with Artemis RL Gym and Claude Sonnet 4 Teacher}},
  author={{Your Name}},
  year={{2025}},
  howpublished={{\\url{{https://huggingface.co/your-username/llama-8b-artemis-claude-teacher}}}},
  note={{Trained using Artemis RL Gym framework}}
}}
```

## Acknowledgments

- Meta AI for the Llama 2 base model
- Anthropic for Claude Sonnet 4 teacher capabilities
- Artemis RL Gym team for the training framework
- HuggingFace for the UltraChat dataset and infrastructure
"""
        
        return model_card

# Example usage
async def upload_example():
    """Example model upload"""
    
    # Assuming trainer is available from training
    uploader = ArtemisModelUploader(trainer, os.getenv("HF_TOKEN"))
    
    repo_url = await uploader.upload_to_hf(
        repo_name="your-username/llama-8b-artemis-claude-teacher",
        private=False,
        include_training_metadata=True
    )
    
    print(f"Model uploaded to: {repo_url}")

if __name__ == "__main__":
    asyncio.run(upload_example())
```
</Tab>
</Tabs>

<Tabs>
<Tab title="Model Upload Script">
```python
# src/upload_model.py
import os
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel
from huggingface_hub import HfApi, create_repo
from safetensors.torch import save_file
import json

class ModelUploader:
    def __init__(self, base_model_name: str, adapter_path: str, hf_token: str):
        self.base_model_name = base_model_name
        self.adapter_path = adapter_path
        self.hf_token = hf_token
        self.api = HfApi(token=hf_token)
    
    def merge_and_save(self, output_path: str):
        """Merge LoRA adapter with base model and save in SafeTensors format"""
        
        print("Loading base model...")
        base_model = AutoModelForCausalLM.from_pretrained(
            self.base_model_name,
            torch_dtype=torch.float16,
            device_map="auto"
        )
        
        print("Loading fine-tuned adapter...")
        model = PeftModel.from_pretrained(base_model, self.adapter_path)
        
        print("Merging adapter with base model...")
        merged_model = model.merge_and_unload()
        
        print("Loading tokenizer...")
        tokenizer = AutoTokenizer.from_pretrained(self.base_model_name)
        
        # Create output directory
        os.makedirs(output_path, exist_ok=True)
        
        print(f"Saving merged model to {output_path}...")
        
        # Save model in SafeTensors format
        merged_model.save_pretrained(
            output_path,
            safe_serialization=True,  # This enables SafeTensors format
            max_shard_size="2GB"
        )
        
        # Save tokenizer
        tokenizer.save_pretrained(output_path)
        
        # Create model card
        self.create_model_card(output_path)
        
        print("Model saved successfully in SafeTensors format!")
    
    def create_model_card(self, model_path: str):
        """Create a comprehensive model card"""
        
        model_card = f"""---
language: en
license: llama2
library_name: transformers
tags:
- llama
- fine-tuned
- instruction-following
- claude-generated-data
- safetensors
base_model: {self.base_model_name}
---

# Llama 8B Fine-tuned with Claude Sonnet 4 Data

This model is a fine-tuned version of [{self.base_model_name}](https://huggingface.co/{self.base_model_name}) using high-quality training data generated by Claude Sonnet 4 (claude-3-5-sonnet-20250514).

## Model Description

- **Base Model**: {self.base_model_name}
- **Fine-tuning Method**: LoRA (Low-Rank Adaptation)
- **Training Data**: Generated using Claude Sonnet 4
- **Format**: SafeTensors
- **Quantization**: 4-bit during training, full precision for inference

## Training Details

### Training Data
- Generated using Claude Sonnet 4 (claude-3-5-sonnet-20250514)
- Topics covered: machine learning, web development, databases, cloud architecture, cybersecurity, etc.
- Format: Instruction-response pairs
- Size: Approximately 50 high-quality examples

### Training Procedure
- **LoRA Configuration**:
  - Rank (r): 16
  - Alpha: 32
  - Dropout: 0.1
  - Target modules: q_proj, v_proj, k_proj, o_proj, gate_proj, up_proj, down_proj
- **Training Parameters**:
  - Epochs: 3
  - Batch size: 4 (with gradient accumulation)
  - Learning rate: 2e-4
  - Max sequence length: 2048

## Usage

```python
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch

# Load model and tokenizer
model_name = "your-username/llama-8b-claude-finetuned"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype=torch.float16,
    device_map="auto"
)

# Generate response
def generate_response(instruction):
    prompt = f'''### Instruction:
{{instruction}}

### Response:
'''
    
    inputs = tokenizer(prompt, return_tensors="pt")
    outputs = model.generate(**inputs, max_new_tokens=512, temperature=0.7)
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    
    return response.split("### Response:")[-1].strip()

# Example usage
response = generate_response("Explain machine learning algorithms with examples")
print(response)
```

## Performance

The model demonstrates improved instruction-following capabilities compared to the base model, particularly in:
- Technical explanations
- Code generation and debugging
- Problem-solving approaches
- Educational content creation

## Limitations

- Inherits limitations from the base Llama 2 8B model
- Training data size is relatively small (50 examples)
- May not perform as well as larger models on complex reasoning tasks

## Ethical Considerations

This model was trained on data generated by Claude Sonnet 4. Users should be aware of potential biases and limitations inherited from both the base model and the training data generation process.

## Citation

If you use this model, please cite:

```bibtex
@misc{{llama-8b-claude-finetuned,
  title={{Llama 8B Fine-tuned with Claude Sonnet 4 Data}},
  author={{Your Name}},
  year={{2025}},
  howpublished={{\\url{{https://huggingface.co/your-username/llama-8b-claude-finetuned}}}}
}}
```
"""
        
        with open(os.path.join(model_path, "README.md"), "w") as f:
            f.write(model_card)
    
    def upload_to_hf(self, local_path: str, repo_name: str, private: bool = False):
        """Upload model to Hugging Face Hub"""
        
        try:
            # Create repository
            print(f"Creating repository: {repo_name}")
            create_repo(
                repo_id=repo_name,
                token=self.hf_token,
                private=private,
                repo_type="model"
            )
            
            # Upload all files
            print("Uploading model files...")
            self.api.upload_folder(
                folder_path=local_path,
                repo_id=repo_name,
                token=self.hf_token,
                commit_message="Upload fine-tuned Llama 8B model with Claude Sonnet 4 data"
            )
            
            print(f"Model successfully uploaded to: https://huggingface.co/{repo_name}")
            
        except Exception as e:
            print(f"Error uploading model: {e}")

# Example usage
def main():
    uploader = ModelUploader(
        base_model_name="meta-llama/Llama-2-8b-chat-hf",
        adapter_path="models/llama-8b-finetuned",
        hf_token=os.getenv("HF_TOKEN")
    )
    
    # Merge and save model
    merged_path = "models/llama-8b-merged"
    uploader.merge_and_save(merged_path)
    
    # Upload to Hugging Face
    repo_name = "your-username/llama-8b-claude-finetuned"  # Change this
    uploader.upload_to_hf(merged_path, repo_name, private=False)

if __name__ == "__main__":
    main()
```
</Tab>

<Tab title="Verification Script">
```python
# verify_upload.py
from huggingface_hub import HfApi
import os

def verify_model_upload(repo_name: str, token: str):
    """Verify that the model was uploaded correctly"""
    
    api = HfApi(token=token)
    
    try:
        # Get repository info
        repo_info = api.repo_info(repo_id=repo_name, repo_type="model")
        
        print(f"Repository: {repo_name}")
        print(f"Created: {repo_info.created_at}")
        print(f"Last modified: {repo_info.last_modified}")
        print(f"Size: {repo_info.size}")
        
        # List files
        files = api.list_repo_files(repo_id=repo_name, repo_type="model")
        print(f"\nFiles in repository:")
        for file in files:
            print(f"  - {file}")
        
        # Check for SafeTensors files
        safetensors_files = [f for f in files if f.endswith('.safetensors')]
        if safetensors_files:
            print(f"\n‚úÖ SafeTensors files found: {len(safetensors_files)}")
        else:
            print("\n‚ùå No SafeTensors files found")
        
        return True
        
    except Exception as e:
        print(f"Error verifying upload: {e}")
        return False

if __name__ == "__main__":
    repo_name = "your-username/llama-8b-claude-finetuned"  # Change this
    token = os.getenv("HF_TOKEN")
    
    verify_model_upload(repo_name, token)
```
</Tab>
</Tabs>

## Step 5: Complete Workflow Script

```python
# main.py - Complete workflow
import asyncio
import os
from dotenv import load_dotenv
from src.dataset_preparation import DatasetPreparator
from src.finetuning import LlamaFineTuner
from src.evaluation import ModelEvaluator
from src.upload_model import ModelUploader
from src.claude_teacher import ClaudeTeacherAgent

async def main():
    """Complete fine-tuning workflow with Claude teacher agent"""
    
    load_dotenv()
    
    print("üöÄ Starting Llama 8B Fine-tuning with Claude Sonnet 4 Teacher Agent")
    
    # Step 1: Prepare dataset from Hugging Face
    print("\nüìä Step 1: Loading and preparing dataset from Hugging Face...")
    
    preparator = DatasetPreparator("HuggingFaceH4/ultrachat_200k")
    data = preparator.load_and_prepare_dataset(max_samples=1000)
    train_file, val_file = preparator.save_dataset(data)
    
    print(f"‚úÖ Dataset prepared: {data['total_samples']} samples")
    
    # Step 2: Initialize Claude teacher agent
    print("\nüßë‚Äçüè´ Step 2: Initializing Claude Sonnet 4 teacher agent...")
    
    teacher_agent = ClaudeTeacherAgent()
    
    # Test teacher agent
    test_instruction = "Explain the concept of machine learning"
    test_response = "Machine learning is a method where computers learn patterns."
    
## Complete Workflow Example

<Tabs>
<Tab title="Full Artemis Workflow">
```python
# complete_workflow.py - End-to-end Artemis training with Claude teacher
import asyncio
import os
from dotenv import load_dotenv

# Artemis imports
from artemis import train_llm_grpo
from artemis.training import PPOTrainer, PPOConfig
from artemis.agents.llm_agent import LLMAgent, LLMAgentConfig
from artemis.environments.dataset_envs.ultrachat import UltraChatEnvironment
from artemis.rl_algorithms.grpo import GRPOConfig
from artemis.utils.monitoring import setup_wandb_logging

# Custom components
from src.claude_teacher import create_claude_teacher
from src.artemis_config import create_ppo_config, create_llm_agent
from src.artemis_evaluation import ArtemisLlamaEvaluator
from src.model_upload import ArtemisModelUploader

async def complete_artemis_workflow():
    """Complete end-to-end workflow using Artemis RL Gym"""
    
    load_dotenv()
    
    print("üöÄ Starting Complete Artemis Workflow: Llama 8B + Claude Teacher")
    print("=" * 60)
    
    # Step 1: Setup and Configuration
    print("\n‚öôÔ∏è Step 1: Artemis Setup and Configuration")
    
    # Setup monitoring
    setup_wandb_logging(
        project="artemis-llama-claude-teacher",
        name="complete-workflow-run",
        tags=["artemis", "llama-8b", "claude-teacher", "ppo"]
    )
    
    # Create configurations
    ppo_config = create_ppo_config()
    llm_agent = create_llm_agent()
    teacher_agent = create_claude_teacher()
    
    print("‚úÖ Artemis components initialized")
    
    # Step 2: High-Level Training with Artemis
    print("\nüèãÔ∏è Step 2: High-Level Training with Artemis")
    
    # Option A: Use Artemis high-level API (Recommended)
    training_results = await train_llm_grpo(
        model_name="meta-llama/Llama-2-8b-chat-hf",
        env_type="ultrachat",  # Built-in UltraChat environment
        num_episodes=500,
        algorithm_config=GRPOConfig(
            learning_rate=1e-4,
            num_reference_policies=3,
            reference_weight=0.1,
            kl_penalty_coefficient=0.02,
            clip_epsilon=0.2,
            use_teacher_evaluation=True
        ),
        reward_model="claude_teacher",  # Use Claude as reward model
        teacher_config={
            "model": "claude-3-5-sonnet-20250514",
            "evaluation_frequency": 50,
            "reward_shaping": True,
            "api_key": os.getenv("ANTHROPIC_API_KEY")
        },
        distributed=True,  # Enable distributed training
        monitoring=True,   # Enable real-time monitoring
        output_dir="./models/llama-8b-artemis-complete",
        save_checkpoints=True,
        checkpoint_frequency=100
    )
    
    print("‚úÖ Artemis training completed!")
    print(f"üìä Results: Final Reward = {training_results.final_reward:.3f}")
    
    # Step 3: Comprehensive Evaluation
    print("\nüìà Step 3: Comprehensive Evaluation")
    
    evaluator = ArtemisLlamaEvaluator(training_results.model_path)
    
    test_prompts = [
        "Explain the fundamentals of machine learning with examples",
        "How do you design a scalable microservices architecture?", 
        "What are the best practices for database optimization?",
        "Describe the principles of secure API development",
        "How to implement effective error handling in production systems?"
    ]
    
    evaluation_results = await evaluator.comprehensive_evaluation(test_prompts)
    
    print(f"üìä Evaluation Summary:")
    print(f"  Overall Score: {evaluation_results['overall_score']:.3f}")
    print(f"  Teacher Reward: {evaluation_results['teacher_evaluation']['average_teacher_reward']:.3f}")
    print(f"  Policy Improvement: {evaluation_results['policy_improvement']['improvement_rate']:.1%}")
    
    # Step 4: Model Upload and Deployment
    print("\nüöÄ Step 4: Model Upload and Deployment")
    
    uploader = ArtemisModelUploader(
        trainer=training_results.trainer,
        hf_token=os.getenv("HF_TOKEN")
    )
    
    repo_url = await uploader.upload_to_hf(
        repo_name="your-username/llama-8b-artemis-complete",
        private=False,
        include_training_metadata=True
    )
    
    print(f"‚úÖ Model uploaded to: {repo_url}")
    
    # Step 5: Final Summary
    print("\nüéâ Workflow Completed Successfully!")
    print("=" * 60)
    print(f"üìà Training Episodes: {training_results.total_episodes}")
    print(f"‚è±Ô∏è Training Time: {training_results.training_time:.2f}s")
    print(f"üèÜ Final Reward: {training_results.final_reward:.3f}")
    print(f"üéØ Teacher Approval: {evaluation_results['teacher_evaluation']['positive_rate']:.1%}")
    print(f"üìä Overall Score: {evaluation_results['overall_score']:.3f}")
    print(f"üîó Model URL: {repo_url}")
    
    return {
        'training_results': training_results,
        'evaluation_results': evaluation_results,
        'model_url': repo_url,
        'success': True
    }

if __name__ == "__main__":
    results = asyncio.run(complete_artemis_workflow())
    print(f"\nüéä All done! Check your results: {results}")
```
</Tab>

<Tab title="Quick Start Script">
```python
# quick_start.py - Minimal Artemis setup
import asyncio
from artemis import train_llm_grpo

async def quick_artemis_training():
    """Quick start with minimal configuration"""
    
    print("‚ö° Quick Artemis Training - Llama 8B + Claude Teacher")
    
    # Minimal configuration
    results = await train_llm_grpo(
        model_name="meta-llama/Llama-2-8b-chat-hf",
        env_type="ultrachat",
        num_episodes=100,  # Small number for quick test
        reward_model="claude_teacher",
        output_dir="./models/llama-8b-quick",
        teacher_config={
            "model": "claude-3-5-sonnet-20250514",
            "evaluation_frequency": 25
        }
    )
    
    print(f"üéâ Quick training done! Final reward: {results.final_reward:.3f}")
    return results

if __name__ == "__main__":
    asyncio.run(quick_artemis_training())
```
</Tab>
</Tabs>

## Troubleshooting

<AccordionGroup>
<Accordion title="Artemis Installation Issues">
**Common Solutions:**
```bash
# If Git LFS issues
git lfs install

# If CUDA issues  
pip install torch --index-url https://download.pytorch.org/whl/cu118

# If Artemis import errors
pip install git+https://github.com/Noema-Research/ArtemisRL-Gym.git --force-reinstall
```
</Accordion>

<Accordion title="CUDA Memory Issues">
**Artemis Memory Optimization:**
```python
# Use Artemis memory management
from artemis.utils.memory import MemoryOptimizer

optimizer = MemoryOptimizer()
optimizer.optimize_for_gpu_memory(
    model_size="8b",
    available_memory="24gb",
    enable_gradient_checkpointing=True,
    use_4bit_quantization=True
)
```
</Accordion>

<Accordion title="Claude API Rate Limiting">
**Artemis Rate Limiting:**
```python
# Artemis handles rate limiting automatically
teacher_config = {
    "model": "claude-3-5-sonnet-20250514",
    "rate_limit": "auto",  # Artemis manages this
    "batch_evaluation": True,
    "async_evaluation": True
}
```
</Accordion>

<Accordion title="Training Convergence Issues">
**Artemis Hyperparameter Tuning:**
```python
# Use Artemis auto-tuning
from artemis.utils.autotuning import AutoHyperparameterTuner

tuner = AutoHyperparameterTuner()
optimal_config = await tuner.optimize(
    base_config=your_config,
    optimization_metric="teacher_reward",
    trials=10
)
```
</Accordion>
</AccordionGroup>

## Best Practices with Artemis

<CardGroup cols={2}>
<Card title="Artemis Configuration" icon="settings">
  - Use built-in environment templates
  - Enable Artemis monitoring for insights
  - Leverage auto-optimization features
  - Configure distributed training for scale
</Card>

<Card title="Teacher Integration" icon="graduation-cap">
  - Set appropriate evaluation frequency
  - Use Artemis reward shaping features
  - Enable async teacher evaluation
  - Monitor teacher-student agreement
</Card>

<Card title="Performance Optimization" icon="rocket">
  - Use Artemis memory optimization
  - Enable gradient compression for distributed
  - Leverage Artemis checkpointing
  - Configure auto-scaling for workloads
</Card>

<Card title="Production Deployment" icon="cloud">
  - Use Artemis model export utilities
  - Enable SafeTensors format
  - Include comprehensive metadata
  - Verify model compatibility
</Card>
</CardGroup>

## Advanced Artemis Features

<Tabs>
<Tab title="Multi-Objective Optimization">
```python
from artemis.rl_algorithms.multi_objective import MultiObjectiveOptimizer

# Optimize for multiple goals
optimizer = MultiObjectiveOptimizer(
    objectives=[
        "teacher_reward",      # Claude teacher satisfaction
        "policy_improvement",  # RL performance
        "safety_score",       # Safety evaluation
        "efficiency"          # Computational efficiency
    ],
    weights=[0.4, 0.3, 0.2, 0.1]
)

results = await optimizer.train(config, agent, environment)
```
</Tab>

<Tab title="Curriculum Learning">
```python
from artemis.rl_algorithms.curriculum import CurriculumLearningManager

# Progressive difficulty training
curriculum = CurriculumLearningManager(
    stages=[
        {"difficulty": "easy", "episodes": 100},
        {"difficulty": "medium", "episodes": 200}, 
        {"difficulty": "hard", "episodes": 200}
    ],
    adaptation_criteria="teacher_reward"
)

results = await curriculum.train(trainer)
```
</Tab>

<Tab title="Fault Tolerance">
```python
from artemis.distributed import FaultTolerantTrainer

# Automatic recovery from failures
ft_trainer = FaultTolerantTrainer(
    base_trainer=trainer,
    checkpoint_frequency=50,
    auto_recovery=True,
    backup_storage="s3://your-bucket/artemis-checkpoints"
)

results = await ft_trainer.train()
```
</Tab>
</Tabs>

## Next Steps

<Steps>
<Step title="Run the Tutorial">
  Execute the complete workflow script and monitor training progress through Artemis dashboards.
</Step>

<Step title="Experiment with Configurations">
  Try different RL algorithms (PPO, DPO, GRPO) and teacher evaluation frequencies to optimize performance.
</Step>

<Step title="Scale with Distributed Training">
  Use Artemis distributed training features to scale across multiple GPUs or nodes for larger models.
</Step>

<Step title="Deploy to Production">
  Leverage Artemis deployment utilities for production-ready model serving and monitoring.
</Step>
</Steps>

Congratulations! You've successfully fine-tuned Llama 8B using **Artemis RL Gym** with Claude Sonnet 4 as a teacher agent. The Artemis framework handled the complex RL training loop, teacher integration, memory optimization, and deployment pipeline, making advanced AI training accessible and efficient.