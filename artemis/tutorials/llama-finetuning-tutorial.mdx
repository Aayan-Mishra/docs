---
title: "Fine-tuning Llama 8B with Claude Sonnet 4 as Teacher Agent"
description: "Complete guide to fine-tuning Meta Llama 8B Instruct using Claude Sonnet 4 as a teacher agent with Hugging Face datasets in a coding environment"
---

This tutorial demonstrates how to fine-tune Meta Llama 8B Instruct using Claude Sonnet 4 as a teacher agent for evaluation and guidance, training on a Hugging Face dataset, then deploy the fine-tuned model to Hugging Face in SafeTensors format.

## Prerequisites

<CardGroup cols={2}>
<Card title="Hardware Requirements" icon="microchip">
  - GPU with 24GB+ VRAM (RTX 4090, A100, H100)
  - 32GB+ System RAM
  - 100GB+ Storage space
</Card>
<Card title="Software Requirements" icon="code">
  - Python 3.9+
  - CUDA 11.8+
  - Git LFS
  - Hugging Face account
</Card>
<Card title="API Access" icon="key">
  - Anthropic API key for Claude Sonnet 4
  - Hugging Face write token
  - Meta Llama access approval
</Card>
<Card title="Knowledge Level" icon="graduation-cap">
  - Intermediate Python
  - Basic ML/PyTorch
  - Command line proficiency
</Card>
</CardGroup>

## Environment Setup

### 1. Install Dependencies

```bash
# Create virtual environment
python -m venv llama_finetune
source llama_finetune/bin/activate  # Linux/Mac
# llama_finetune\Scripts\activate  # Windows

# Install core dependencies
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
pip install transformers>=4.36.0
pip install datasets
pip install peft
pip install bitsandbytes
pip install accelerate
pip install safetensors
pip install huggingface_hub
pip install anthropic
pip install trl
pip install wandb  # Optional for logging
```

### 2. Environment Variables

Create a `.env` file in your project directory:

```bash
# .env file
ANTHROPIC_API_KEY=your_anthropic_api_key_here
HF_TOKEN=your_huggingface_token_here
WANDB_API_KEY=your_wandb_key_here  # Optional
```

### 3. Hugging Face Authentication

```bash
# Login to Hugging Face
huggingface-cli login --token $HF_TOKEN

# Accept Meta Llama license (required)
# Visit: https://huggingface.co/meta-llama/Llama-2-8b-chat-hf
```

## Project Structure

```
llama-finetuning/
‚îú‚îÄ‚îÄ .env
‚îú‚îÄ‚îÄ requirements.txt
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ data_generation.py
‚îÇ   ‚îú‚îÄ‚îÄ finetuning.py
‚îÇ   ‚îú‚îÄ‚îÄ evaluation.py
‚îÇ   ‚îî‚îÄ‚îÄ upload_model.py
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îú‚îÄ‚îÄ training_data.json
‚îÇ   ‚îî‚îÄ‚îÄ validation_data.json
‚îú‚îÄ‚îÄ models/
‚îÇ   ‚îî‚îÄ‚îÄ llama-8b-finetuned/
‚îî‚îÄ‚îÄ configs/
    ‚îî‚îÄ‚îÄ training_config.yaml
```

## Step 1: Dataset Preparation from Hugging Face

<Tabs>
<Tab title="Dataset Loader">
```python
# src/dataset_preparation.py
import os
from datasets import load_dataset, Dataset
from typing import Dict, List
import json

class DatasetPreparator:
    def __init__(self, dataset_name: str = "HuggingFaceH4/ultrachat_200k"):
        """
        Initialize with a Hugging Face dataset
        
        Popular options:
        - "HuggingFaceH4/ultrachat_200k": High-quality conversational data
        - "tatsu-lab/alpaca": Instruction following dataset
        - "OpenAssistant/oasst1": Open Assistant conversations
        - "WizardLM/WizardLM_evol_instruct_V2_196k": Evolved instructions
        """
        self.dataset_name = dataset_name
    
    def load_and_prepare_dataset(self, split: str = "train_sft", max_samples: int = 5000) -> Dict:
        """Load and prepare dataset from Hugging Face"""
        
        print(f"Loading dataset: {self.dataset_name}")
        
        # Load dataset
        dataset = load_dataset(self.dataset_name, split=split)
        
        # Take a subset for faster training
        if len(dataset) > max_samples:
            dataset = dataset.select(range(max_samples))
            print(f"Selected {max_samples} samples from {len(dataset)} total")
        
        # Convert to our format
        formatted_data = []
        
        for item in dataset:
            # Handle different dataset formats
            if 'messages' in item:
                # UltraChat format
                messages = item['messages']
                if len(messages) >= 2:
                    instruction = messages[0]['content']
                    response = messages[1]['content']
                    
                    formatted_data.append({
                        'instruction': instruction,
                        'output': response,
                        'source': self.dataset_name
                    })
            
            elif 'instruction' in item and 'output' in item:
                # Alpaca format
                instruction = item['instruction']
                if 'input' in item and item['input']:
                    instruction += f"\n\nInput: {item['input']}"
                
                formatted_data.append({
                    'instruction': instruction,
                    'output': item['output'],
                    'source': self.dataset_name
                })
            
            elif 'prompt' in item and 'response' in item:
                # Generic format
                formatted_data.append({
                    'instruction': item['prompt'],
                    'output': item['response'],
                    'source': self.dataset_name
                })
        
        # Split into train/validation
        split_point = int(len(formatted_data) * 0.8)
        train_data = formatted_data[:split_point]
        val_data = formatted_data[split_point:]
        
        return {
            'train': train_data,
            'validation': val_data,
            'total_samples': len(formatted_data)
        }
    
    def save_dataset(self, data: Dict, output_dir: str = "data"):
        """Save prepared dataset to files"""
        
        os.makedirs(output_dir, exist_ok=True)
        
        # Save training data
        with open(f"{output_dir}/training_data.json", "w") as f:
            json.dump(data['train'], f, indent=2)
        
        # Save validation data
        with open(f"{output_dir}/validation_data.json", "w") as f:
            json.dump(data['validation'], f, indent=2)
        
        print(f"Saved {len(data['train'])} training and {len(data['validation'])} validation samples")
        
        return f"{output_dir}/training_data.json", f"{output_dir}/validation_data.json"

# Example usage
def main():
    # Initialize dataset preparator
    preparator = DatasetPreparator("HuggingFaceH4/ultrachat_200k")
    
    # Load and prepare dataset
    data = preparator.load_and_prepare_dataset(max_samples=1000)
    
    # Save to files
    train_file, val_file = preparator.save_dataset(data)
    
    print(f"Dataset preparation completed!")
    print(f"Training file: {train_file}")
    print(f"Validation file: {val_file}")

if __name__ == "__main__":
    main()
```
</Tab>

<Tab title="Alternative Datasets">
```python
# Popular HF datasets for instruction tuning

RECOMMENDED_DATASETS = {
    "ultrachat": {
        "name": "HuggingFaceH4/ultrachat_200k",
        "split": "train_sft",
        "description": "High-quality multi-turn conversations",
        "size": "200k samples"
    },
    
    "alpaca": {
        "name": "tatsu-lab/alpaca",
        "split": "train",
        "description": "Instruction-following dataset",
        "size": "52k samples"
    },
    
    "openassistant": {
        "name": "OpenAssistant/oasst1",
        "split": "train",
        "description": "Human-generated conversations",
        "size": "161k samples"
    },
    
    "wizardlm": {
        "name": "WizardLM/WizardLM_evol_instruct_V2_196k",
        "split": "train",
        "description": "Evolved instruction dataset",
        "size": "196k samples"
    },
    
    "dolly": {
        "name": "databricks/databricks-dolly-15k",
        "split": "train",
        "description": "Human-generated instruction dataset",
        "size": "15k samples"
    }
}

# Load different datasets
def load_dataset_by_name(dataset_key: str):
    config = RECOMMENDED_DATASETS.get(dataset_key)
    if not config:
        raise ValueError(f"Unknown dataset: {dataset_key}")
    
    preparator = DatasetPreparator(config["name"])
    return preparator.load_and_prepare_dataset(split=config["split"])
```
</Tab>
</Tabs>

## Step 2: Claude Sonnet 4 as Teacher Agent

<Tabs>
<Tab title="Teacher Agent">
```python
# src/claude_teacher.py
import os
import asyncio
from typing import Dict, List, Any
from anthropic import Anthropic
from dotenv import load_dotenv

load_dotenv()

class ClaudeTeacherAgent:
    def __init__(self):
        self.client = Anthropic(api_key=os.getenv("ANTHROPIC_API_KEY"))
        self.model = "claude-3-5-sonnet-20250514"
    
    async def evaluate_response(self, instruction: str, student_response: str, reference_response: str = None) -> Dict[str, Any]:
        """Evaluate student model response using Claude as teacher"""
        
        system_prompt = """You are an expert AI teacher evaluating responses from a student language model. 
        
        Provide detailed, constructive feedback focusing on:
        1. Correctness and accuracy
        2. Completeness and thoroughness
        3. Clarity and coherence
        4. Helpfulness and practical value
        5. Safety and appropriateness
        
        Rate each aspect on a scale of 1-10 and provide an overall score.
        Give specific suggestions for improvement."""
        
        user_prompt = f"""
Instruction: {instruction}

Student Response: {student_response}

{f"Reference Response: {reference_response}" if reference_response else ""}

Please evaluate this response and provide detailed feedback."""
        
        try:
            response = await self.client.messages.create(
                model=self.model,
                max_tokens=1024,
                temperature=0.3,  # Lower temperature for consistent evaluation
                system=system_prompt,
                messages=[{"role": "user", "content": user_prompt}]
            )
            
            evaluation = response.content[0].text
            
            # Extract numerical scores (simplified parsing)
            # In practice, you might want to use structured output
            return {
                "evaluation": evaluation,
                "teacher_model": self.model,
                "instruction": instruction,
                "student_response": student_response
            }
            
        except Exception as e:
            print(f"Error evaluating response: {e}")
            return None
    
    async def generate_reward_signal(self, instruction: str, student_response: str) -> Dict[str, Any]:
        """Generate reward signal for RL training"""
        
        system_prompt = """You are a reward model for reinforcement learning training. 
        
        Evaluate the student response and provide a reward score between -1.0 and 1.0:
        - 1.0: Excellent response, exceeds expectations
        - 0.5-0.9: Good response with minor issues
        - 0.0-0.4: Adequate response but needs improvement
        - -0.1 to -0.5: Poor response with significant issues
        - -0.6 to -1.0: Harmful, incorrect, or inappropriate response
        
        Provide the score and brief reasoning."""
        
        user_prompt = f"""
Instruction: {instruction}

Student Response: {student_response}

Provide a reward score (-1.0 to 1.0) and brief reasoning."""
        
        try:
            response = await self.client.messages.create(
                model=self.model,
                max_tokens=256,
                temperature=0.2,
                system=system_prompt,
                messages=[{"role": "user", "content": user_prompt}]
            )
            
            content = response.content[0].text
            
            # Simple parsing to extract score (in practice, use structured output)
            # This is a simplified version - you'd want more robust parsing
            score = 0.0
            try:
                # Look for patterns like "Score: 0.8" or "0.7/1.0"
                import re
                score_match = re.search(r'[-]?\d+\.?\d*', content)
                if score_match:
                    score = float(score_match.group())
                    score = max(-1.0, min(1.0, score))  # Clamp to valid range
            except:
                score = 0.0
            
            return {
                "reward": score,
                "reasoning": content,
                "teacher_model": self.model
            }
            
        except Exception as e:
            print(f"Error generating reward: {e}")
            return {"reward": 0.0, "reasoning": "Error occurred", "teacher_model": self.model}
    
    async def provide_guidance(self, instruction: str, student_response: str) -> Dict[str, str]:
        """Provide guidance for improving the response"""
        
        system_prompt = """You are a helpful AI tutor. Analyze the student's response and provide specific, actionable guidance for improvement.
        
        Focus on:
        1. What the student did well
        2. Specific areas for improvement
        3. Concrete suggestions for better responses
        4. Examples of improved phrasing or content"""
        
        user_prompt = f"""
Instruction: {instruction}

Student Response: {student_response}

Please provide specific guidance for improvement."""
        
        try:
            response = await self.client.messages.create(
                model=self.model,
                max_tokens=512,
                temperature=0.4,
                system=system_prompt,
                messages=[{"role": "user", "content": user_prompt}]
            )
            
            return {
                "guidance": response.content[0].text,
                "teacher_model": self.model
            }
            
        except Exception as e:
            print(f"Error providing guidance: {e}")
            return {"guidance": "Error occurred", "teacher_model": self.model}

# Example usage
async def main():
    teacher = ClaudeTeacherAgent()
    
    # Example evaluation
    instruction = "Explain the concept of machine learning algorithms"
    student_response = "Machine learning is when computers learn things automatically."
    
    # Get evaluation
    evaluation = await teacher.evaluate_response(instruction, student_response)
    print("Evaluation:", evaluation["evaluation"] if evaluation else "Failed")
    
    # Get reward signal
    reward = await teacher.generate_reward_signal(instruction, student_response)
    print(f"Reward: {reward['reward']}")
    print(f"Reasoning: {reward['reasoning']}")
    
    # Get guidance
    guidance = await teacher.provide_guidance(instruction, student_response)
    print("Guidance:", guidance["guidance"])

if __name__ == "__main__":
    asyncio.run(main())
```
</Tab>

<Tab title="Integration with Training">
```python
# src/teacher_guided_training.py
from transformers import TrainerCallback
import asyncio
import json
from .claude_teacher import ClaudeTeacherAgent

class TeacherGuidedCallback(TrainerCallback):
    """Custom callback to integrate Claude teacher evaluation during training"""
    
    def __init__(self, teacher_agent: ClaudeTeacherAgent, eval_dataset, tokenizer, eval_frequency=100):
        self.teacher_agent = teacher_agent
        self.eval_dataset = eval_dataset
        self.tokenizer = tokenizer
        self.eval_frequency = eval_frequency
        self.evaluation_history = []
    
    def on_step_end(self, args, state, control, model=None, **kwargs):
        """Evaluate model with teacher agent periodically"""
        
        if state.global_step % self.eval_frequency == 0:
            print(f"\nüçé Teacher evaluation at step {state.global_step}")
            
            # Run async evaluation
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
            
            try:
                evaluation_results = loop.run_until_complete(
                    self._evaluate_with_teacher(model, state.global_step)
                )
                
                # Log results
                avg_reward = sum(r['reward'] for r in evaluation_results) / len(evaluation_results)
                print(f"Average teacher reward: {avg_reward:.3f}")
                
                # Store for analysis
                self.evaluation_history.append({
                    'step': state.global_step,
                    'avg_reward': avg_reward,
                    'evaluations': evaluation_results
                })
                
            finally:
                loop.close()
    
    async def _evaluate_with_teacher(self, model, step, num_samples=5):
        """Evaluate a few samples with the teacher"""
        
        results = []
        
        # Select random samples from eval dataset
        import random
        sample_indices = random.sample(range(len(self.eval_dataset)), min(num_samples, len(self.eval_dataset)))
        
        for idx in sample_indices:
            sample = self.eval_dataset[idx]
            instruction = sample['instruction']
            
            # Generate response with current model
            student_response = self._generate_response(model, instruction)
            
            # Get teacher evaluation
            reward_data = await self.teacher_agent.generate_reward_signal(instruction, student_response)
            
            results.append({
                'instruction': instruction,
                'student_response': student_response,
                'reward': reward_data['reward'],
                'reasoning': reward_data['reasoning']
            })
        
        return results
    
    def _generate_response(self, model, instruction, max_length=512):
        """Generate response using current model state"""
        
        prompt = f"""### Instruction:
{instruction}

### Response:
"""
        
        inputs = self.tokenizer(prompt, return_tensors="pt").to(model.device)
        
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=max_length,
                temperature=0.7,
                do_sample=True,
                pad_token_id=self.tokenizer.eos_token_id,
                eos_token_id=self.tokenizer.eos_token_id,
            )
        
        response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
        
        # Extract only the response part
        response_start = response.find("### Response:") + len("### Response:")
        return response[response_start:].strip()
    
    def save_evaluation_history(self, path="teacher_evaluations.json"):
        """Save all teacher evaluations for analysis"""
        with open(path, 'w') as f:
            json.dump(self.evaluation_history, f, indent=2)
```
</Tab>
</Tabs>

<Tabs>
<Tab title="Fine-tuning Script">
```python
# src/finetuning.py
import os
import json
import torch
from datasets import Dataset
from transformers import (
    AutoTokenizer, 
    AutoModelForCausalLM,
    TrainingArguments,
    Trainer,
    DataCollatorForLanguageModeling
)
from peft import LoraConfig, get_peft_model, TaskType
from trl import SFTTrainer
import wandb

class LlamaFineTuner:
    def __init__(self, model_name: str = "meta-llama/Llama-2-8b-chat-hf"):
        self.model_name = model_name
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        
        # Initialize tokenizer
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        self.tokenizer.pad_token = self.tokenizer.eos_token
        self.tokenizer.padding_side = "right"
        
        # Load model with 4-bit quantization for memory efficiency
        self.model = AutoModelForCausalLM.from_pretrained(
            model_name,
            torch_dtype=torch.float16,
            device_map="auto",
            load_in_4bit=True,
            bnb_4bit_quant_type="nf4",
            bnb_4bit_compute_dtype=torch.float16,
            bnb_4bit_use_double_quant=True,
        )
        
        # Configure LoRA
        self.lora_config = LoraConfig(
            task_type=TaskType.CAUSAL_LM,
            inference_mode=False,
            r=16,  # Rank
            lora_alpha=32,  # Alpha parameter
            lora_dropout=0.1,
            target_modules=["q_proj", "v_proj", "k_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]
        )
        
        # Apply LoRA
        self.model = get_peft_model(self.model, self.lora_config)
        self.model.print_trainable_parameters()
    
    def load_dataset(self, file_path: str) -> Dataset:
        """Load and format dataset"""
        with open(file_path, 'r') as f:
            data = json.load(f)
        
        # Format data for instruction tuning
        formatted_data = []
        for item in data:
            formatted_text = f"""### Instruction:
{item['instruction']}

### Response:
{item['output']}"""
            formatted_data.append({"text": formatted_text})
        
        return Dataset.from_list(formatted_data)
    
    def tokenize_function(self, examples):
        """Tokenize the dataset"""
        return self.tokenizer(
            examples["text"],
            truncation=True,
            padding="max_length",
            max_length=2048,
            return_tensors="pt"
        )
    
    def train(self, train_file: str, val_file: str, output_dir: str = "models/llama-8b-finetuned"):
        """Fine-tune the model"""
        
        # Load datasets
        train_dataset = self.load_dataset(train_file)
        val_dataset = self.load_dataset(val_file)
        
        # Tokenize datasets
        train_dataset = train_dataset.map(self.tokenize_function, batched=True)
        val_dataset = val_dataset.map(self.tokenize_function, batched=True)
        
        # Training arguments
        training_args = TrainingArguments(
            output_dir=output_dir,
            num_train_epochs=3,
            per_device_train_batch_size=4,
            per_device_eval_batch_size=4,
            gradient_accumulation_steps=4,
            warmup_steps=100,
            max_steps=1000,
            learning_rate=2e-4,
            fp16=True,
            logging_steps=10,
            evaluation_strategy="steps",
            eval_steps=100,
            save_steps=200,
            save_total_limit=3,
            load_best_model_at_end=True,
            metric_for_best_model="eval_loss",
            greater_is_better=False,
            dataloader_pin_memory=False,
            remove_unused_columns=False,
            report_to="wandb" if os.getenv("WANDB_API_KEY") else None,
        )
        
        # Initialize Weights & Biases (optional)
        if os.getenv("WANDB_API_KEY"):
            wandb.init(project="llama-finetuning", name="llama-8b-claude-data")
        
        # Data collator
        data_collator = DataCollatorForLanguageModeling(
            tokenizer=self.tokenizer,
            mlm=False,
        )
        
        # Initialize trainer
        trainer = SFTTrainer(
            model=self.model,
            train_dataset=train_dataset,
            eval_dataset=val_dataset,
            tokenizer=self.tokenizer,
            data_collator=data_collator,
            args=training_args,
            max_seq_length=2048,
            dataset_text_field="text",
        )
        
        # Train the model
        print("Starting training...")
        trainer.train()
        
        # Save the model
        trainer.save_model()
        self.tokenizer.save_pretrained(output_dir)
        
        print(f"Model saved to {output_dir}")
        
        return trainer

# Example usage
def main():
    # Initialize fine-tuner
    fine_tuner = LlamaFineTuner()
    
    # Train the model
    trainer = fine_tuner.train(
        train_file="data/training_data.json",
        val_file="data/validation_data.json"
    )
    
    print("Fine-tuning completed!")

if __name__ == "__main__":
    main()
```
</Tab>

<Tab title="Configuration">
```yaml
# configs/training_config.yaml
model:
  name: "meta-llama/Llama-2-8b-chat-hf"
  max_length: 2048

lora:
  r: 16
  alpha: 32
  dropout: 0.1
  target_modules:
    - "q_proj"
    - "v_proj"
    - "k_proj"
    - "o_proj"
    - "gate_proj"
    - "up_proj"
    - "down_proj"

training:
  epochs: 3
  batch_size: 4
  gradient_accumulation_steps: 4
  learning_rate: 2e-4
  warmup_steps: 100
  max_steps: 1000
  eval_steps: 100
  save_steps: 200

quantization:
  load_in_4bit: true
  bnb_4bit_quant_type: "nf4"
  bnb_4bit_compute_dtype: "float16"
  bnb_4bit_use_double_quant: true
```
</Tab>
</Tabs>

## Step 3: Model Fine-tuning with Teacher Guidance

<Tabs>
<Tab title="Enhanced Fine-tuning Script">
```python
# src/finetuning.py
import os
import json
import torch
from datasets import Dataset
from transformers import (
    AutoTokenizer, 
    AutoModelForCausalLM,
    TrainingArguments,
    Trainer,
    DataCollatorForLanguageModeling
)
from peft import LoraConfig, get_peft_model, TaskType
from trl import SFTTrainer
import wandb
from .claude_teacher import ClaudeTeacherAgent
from .teacher_guided_training import TeacherGuidedCallback

class LlamaFineTuner:
    def __init__(self, model_name: str = "meta-llama/Llama-2-8b-chat-hf"):
        self.model_name = model_name
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        
        # Initialize teacher agent
        self.teacher_agent = ClaudeTeacherAgent()
        
        # Initialize tokenizer
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        self.tokenizer.pad_token = self.tokenizer.eos_token
        self.tokenizer.padding_side = "right"
        
        # Load model with 4-bit quantization for memory efficiency
        self.model = AutoModelForCausalLM.from_pretrained(
            model_name,
            torch_dtype=torch.float16,
            device_map="auto",
            load_in_4bit=True,
            bnb_4bit_quant_type="nf4",
            bnb_4bit_compute_dtype=torch.float16,
            bnb_4bit_use_double_quant=True,
        )
        
        # Configure LoRA
        self.lora_config = LoraConfig(
            task_type=TaskType.CAUSAL_LM,
            inference_mode=False,
            r=16,  # Rank
            lora_alpha=32,  # Alpha parameter
            lora_dropout=0.1,
            target_modules=["q_proj", "v_proj", "k_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]
        )
        
        # Apply LoRA
        self.model = get_peft_model(self.model, self.lora_config)
        self.model.print_trainable_parameters()
    
    def load_dataset(self, file_path: str) -> Dataset:
        """Load and format dataset"""
        with open(file_path, 'r') as f:
            data = json.load(f)
        
        # Format data for instruction tuning
        formatted_data = []
        for item in data:
            formatted_text = f"""### Instruction:
{item['instruction']}

### Response:
{item['output']}"""
            formatted_data.append({
                "text": formatted_text,
                "instruction": item['instruction'],
                "output": item['output']
            })
        
        return Dataset.from_list(formatted_data)
    
    def tokenize_function(self, examples):
        """Tokenize the dataset"""
        return self.tokenizer(
            examples["text"],
            truncation=True,
            padding="max_length",
            max_length=2048,
            return_tensors="pt"
        )
    
    def train(self, train_file: str, val_file: str, output_dir: str = "models/llama-8b-finetuned"):
        """Fine-tune the model with teacher guidance"""
        
        # Load datasets
        train_dataset = self.load_dataset(train_file)
        val_dataset = self.load_dataset(val_file)
        
        # Tokenize datasets
        train_dataset = train_dataset.map(self.tokenize_function, batched=True)
        val_dataset = val_dataset.map(self.tokenize_function, batched=True)
        
        # Training arguments
        training_args = TrainingArguments(
            output_dir=output_dir,
            num_train_epochs=3,
            per_device_train_batch_size=4,
            per_device_eval_batch_size=4,
            gradient_accumulation_steps=4,
            warmup_steps=100,
            max_steps=1000,
            learning_rate=2e-4,
            fp16=True,
            logging_steps=10,
            evaluation_strategy="steps",
            eval_steps=100,
            save_steps=200,
            save_total_limit=3,
            load_best_model_at_end=True,
            metric_for_best_model="eval_loss",
            greater_is_better=False,
            dataloader_pin_memory=False,
            remove_unused_columns=False,
            report_to="wandb" if os.getenv("WANDB_API_KEY") else None,
        )
        
        # Initialize Weights & Biases (optional)
        if os.getenv("WANDB_API_KEY"):
            wandb.init(
                project="llama-finetuning-claude-teacher", 
                name="llama-8b-hf-data-claude-guidance"
            )
        
        # Data collator
        data_collator = DataCollatorForLanguageModeling(
            tokenizer=self.tokenizer,
            mlm=False,
        )
        
        # Create teacher guidance callback
        teacher_callback = TeacherGuidedCallback(
            teacher_agent=self.teacher_agent,
            eval_dataset=val_dataset,
            tokenizer=self.tokenizer,
            eval_frequency=100  # Evaluate every 100 steps
        )
        
        # Initialize trainer
        trainer = SFTTrainer(
            model=self.model,
            train_dataset=train_dataset,
            eval_dataset=val_dataset,
            tokenizer=self.tokenizer,
            data_collator=data_collator,
            args=training_args,
            max_seq_length=2048,
            dataset_text_field="text",
            callbacks=[teacher_callback]  # Add teacher guidance
        )
        
        # Train the model
        print("üöÄ Starting training with Claude teacher guidance...")
        trainer.train()
        
        # Save the model
        trainer.save_model()
        self.tokenizer.save_pretrained(output_dir)
        
        # Save teacher evaluation history
        teacher_callback.save_evaluation_history(
            os.path.join(output_dir, "teacher_evaluations.json")
        )
        
        print(f"Model saved to {output_dir}")
        print("Teacher evaluation history saved")
        
        return trainer

# Example usage
def main():
    # Initialize fine-tuner
    fine_tuner = LlamaFineTuner()
    
    # Train the model
    trainer = fine_tuner.train(
        train_file="data/training_data.json",
        val_file="data/validation_data.json"
    )
    
    print("Fine-tuning with teacher guidance completed!")

if __name__ == "__main__":
    main()
```
</Tab>

<Tab title="Real-time Evaluation">
```python
# src/realtime_evaluation.py
import asyncio
import matplotlib.pyplot as plt
from typing import List, Dict
import json

class RealTimeEvaluator:
    """Monitor training progress with real-time teacher feedback"""
    
    def __init__(self, teacher_agent, model, tokenizer):
        self.teacher_agent = teacher_agent
        self.model = model
        self.tokenizer = tokenizer
        self.evaluation_history = []
    
    async def continuous_evaluation(self, test_prompts: List[str], interval_steps: int = 50):
        """Continuously evaluate model during training"""
        
        while True:
            # Wait for interval
            await asyncio.sleep(interval_steps)
            
            # Evaluate on test prompts
            results = []
            for prompt in test_prompts:
                response = self._generate_response(prompt)
                reward = await self.teacher_agent.generate_reward_signal(prompt, response)
                
                results.append({
                    'prompt': prompt,
                    'response': response,
                    'reward': reward['reward'],
                    'reasoning': reward['reasoning']
                })
            
            # Calculate metrics
            avg_reward = sum(r['reward'] for r in results) / len(results)
            
            # Store results
            self.evaluation_history.append({
                'timestamp': asyncio.get_event_loop().time(),
                'avg_reward': avg_reward,
                'results': results
            })
            
            # Log progress
            print(f"üìä Current average teacher reward: {avg_reward:.3f}")
    
    def plot_progress(self, save_path: str = "training_progress.png"):
        """Plot training progress based on teacher feedback"""
        
        if not self.evaluation_history:
            print("No evaluation history to plot")
            return
        
        timestamps = [e['timestamp'] for e in self.evaluation_history]
        rewards = [e['avg_reward'] for e in self.evaluation_history]
        
        plt.figure(figsize=(10, 6))
        plt.plot(timestamps, rewards, 'b-', linewidth=2, label='Teacher Reward')
        plt.xlabel('Training Time')
        plt.ylabel('Average Teacher Reward')
        plt.title('Model Performance During Training (Teacher Evaluated)')
        plt.legend()
        plt.grid(True, alpha=0.3)
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"Progress plot saved to {save_path}")
    
    def _generate_response(self, prompt: str, max_length: int = 256):
        """Generate response for evaluation"""
        
        formatted_prompt = f"""### Instruction:
{prompt}

### Response:
"""
        
        inputs = self.tokenizer(formatted_prompt, return_tensors="pt").to(self.model.device)
        
        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=max_length,
                temperature=0.7,
                do_sample=True,
                pad_token_id=self.tokenizer.eos_token_id,
            )
        
        response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
        response_start = response.find("### Response:") + len("### Response:")
        return response[response_start:].strip()
```
</Tab>
</Tabs>

```python
# src/evaluation.py
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel
import json

class ModelEvaluator:
    def __init__(self, base_model_name: str, adapter_path: str):
        self.tokenizer = AutoTokenizer.from_pretrained(base_model_name)
        self.tokenizer.pad_token = self.tokenizer.eos_token
        
        # Load base model
        base_model = AutoModelForCausalLM.from_pretrained(
            base_model_name,
            torch_dtype=torch.float16,
            device_map="auto"
        )
        
        # Load fine-tuned adapter
        self.model = PeftModel.from_pretrained(base_model, adapter_path)
        self.model.eval()
    
    def generate_response(self, instruction: str, max_length: int = 512) -> str:
        """Generate response using the fine-tuned model"""
        
        prompt = f"""### Instruction:
{instruction}

### Response:
"""
        
        inputs = self.tokenizer(prompt, return_tensors="pt").to(self.model.device)
        
        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=max_length,
                temperature=0.7,
                do_sample=True,
                pad_token_id=self.tokenizer.eos_token_id,
                eos_token_id=self.tokenizer.eos_token_id,
            )
        
        response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
        
        # Extract only the response part
        response_start = response.find("### Response:") + len("### Response:")
        return response[response_start:].strip()
    
    def evaluate_on_test_set(self, test_file: str) -> Dict:
        """Evaluate model on test set"""
        
        with open(test_file, 'r') as f:
            test_data = json.load(f)
        
        results = []
        
        for item in test_data:
            instruction = item['instruction']
            expected = item['output']
            
            generated = self.generate_response(instruction)
            
            results.append({
                'instruction': instruction,
                'expected': expected,
                'generated': generated
            })
        
        return results

# Example usage
def main():
    evaluator = ModelEvaluator(
        base_model_name="meta-llama/Llama-2-8b-chat-hf",
        adapter_path="models/llama-8b-finetuned"
    )
    
    # Test single generation
    test_instruction = "Explain the concept of machine learning algorithms in detail with practical examples."
    response = evaluator.generate_response(test_instruction)
    
    print("Instruction:", test_instruction)
    print("Response:", response)

if __name__ == "__main__":
    main()
```

## Step 4: Post-Training Evaluation with Teacher Agent

```python
# src/evaluation.py
import torch
import asyncio
import json
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel
from .claude_teacher import ClaudeTeacherAgent

class ModelEvaluator:
    def __init__(self, base_model_name: str, adapter_path: str):
        self.tokenizer = AutoTokenizer.from_pretrained(base_model_name)
        self.tokenizer.pad_token = self.tokenizer.eos_token
        
        # Load base model
        base_model = AutoModelForCausalLM.from_pretrained(
            base_model_name,
            torch_dtype=torch.float16,
            device_map="auto"
        )
        
        # Load fine-tuned adapter
        self.model = PeftModel.from_pretrained(base_model, adapter_path)
        self.model.eval()
        
        # Initialize teacher agent for evaluation
        self.teacher_agent = ClaudeTeacherAgent()
    
    def generate_response(self, instruction: str, max_length: int = 512) -> str:
        """Generate response using the fine-tuned model"""
        
        prompt = f"""### Instruction:
{instruction}

### Response:
"""
        
        inputs = self.tokenizer(prompt, return_tensors="pt").to(self.model.device)
        
        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=max_length,
                temperature=0.7,
                do_sample=True,
                pad_token_id=self.tokenizer.eos_token_id,
                eos_token_id=self.tokenizer.eos_token_id,
            )
        
        response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
        
        # Extract only the response part
        response_start = response.find("### Response:") + len("### Response:")
        return response[response_start:].strip()
    
    async def comprehensive_evaluation(self, test_file: str) -> Dict:
        """Comprehensive evaluation using Claude teacher agent"""
        
        with open(test_file, 'r') as f:
            test_data = json.load(f)
        
        results = []
        total_reward = 0
        
        print(f"üß™ Evaluating {len(test_data)} examples with Claude teacher...")
        
        for i, item in enumerate(test_data):
            instruction = item['instruction']
            expected = item['output'] if 'output' in item else None
            
            # Generate response with fine-tuned model
            generated = self.generate_response(instruction)
            
            # Get teacher evaluation
            evaluation = await self.teacher_agent.evaluate_response(
                instruction, generated, expected
            )
            
            # Get reward signal
            reward_data = await self.teacher_agent.generate_reward_signal(
                instruction, generated
            )
            
            # Get improvement guidance
            guidance = await self.teacher_agent.provide_guidance(
                instruction, generated
            )
            
            result = {
                'instruction': instruction,
                'expected': expected,
                'generated': generated,
                'teacher_evaluation': evaluation['evaluation'] if evaluation else None,
                'reward': reward_data['reward'],
                'reward_reasoning': reward_data['reasoning'],
                'guidance': guidance['guidance']
            }
            
            results.append(result)
            total_reward += reward_data['reward']
            
            print(f"Evaluated {i+1}/{len(test_data)} (Reward: {reward_data['reward']:.2f})")
            
            # Rate limiting
            await asyncio.sleep(1)
        
        # Calculate summary statistics
        avg_reward = total_reward / len(results)
        positive_rewards = sum(1 for r in results if r['reward'] > 0)
        excellent_responses = sum(1 for r in results if r['reward'] > 0.7)
        
        summary = {
            'total_examples': len(results),
            'average_reward': avg_reward,
            'positive_rate': positive_rewards / len(results),
            'excellent_rate': excellent_responses / len(results),
            'detailed_results': results
        }
        
        return summary
    
    async def compare_with_base_model(self, base_model_name: str, test_prompts: List[str]) -> Dict:
        """Compare fine-tuned model with base model using teacher evaluation"""
        
        # Load base model for comparison
        base_tokenizer = AutoTokenizer.from_pretrained(base_model_name)
        base_tokenizer.pad_token = base_tokenizer.eos_token
        
        base_model = AutoModelForCausalLM.from_pretrained(
            base_model_name,
            torch_dtype=torch.float16,
            device_map="auto"
        )
        
        comparison_results = []
        
        print(f"üîç Comparing fine-tuned vs base model on {len(test_prompts)} prompts...")
        
        for prompt in test_prompts:
            # Generate with fine-tuned model
            finetuned_response = self.generate_response(prompt)
            
            # Generate with base model
            base_prompt = f"""### Instruction:
{prompt}

### Response:
"""
            
            base_inputs = base_tokenizer(base_prompt, return_tensors="pt").to(base_model.device)
            
            with torch.no_grad():
                base_outputs = base_model.generate(
                    **base_inputs,
                    max_new_tokens=512,
                    temperature=0.7,
                    do_sample=True,
                    pad_token_id=base_tokenizer.eos_token_id,
                )
            
            base_response = base_tokenizer.decode(base_outputs[0], skip_special_tokens=True)
            base_response_start = base_response.find("### Response:") + len("### Response:")
            base_response = base_response[base_response_start:].strip()
            
            # Get teacher evaluation for both
            finetuned_reward = await self.teacher_agent.generate_reward_signal(prompt, finetuned_response)
            base_reward = await self.teacher_agent.generate_reward_signal(prompt, base_response)
            
            comparison_results.append({
                'prompt': prompt,
                'finetuned_response': finetuned_response,
                'base_response': base_response,
                'finetuned_reward': finetuned_reward['reward'],
                'base_reward': base_reward['reward'],
                'improvement': finetuned_reward['reward'] - base_reward['reward']
            })
            
            await asyncio.sleep(1)  # Rate limiting
        
        # Calculate improvement metrics
        improvements = [r['improvement'] for r in comparison_results]
        avg_improvement = sum(improvements) / len(improvements)
        improved_count = sum(1 for imp in improvements if imp > 0)
        
        return {
            'average_improvement': avg_improvement,
            'improvement_rate': improved_count / len(comparison_results),
            'detailed_comparisons': comparison_results
        }

# Example usage
async def main():
    evaluator = ModelEvaluator(
        base_model_name="meta-llama/Llama-2-8b-chat-hf",
        adapter_path="models/llama-8b-finetuned"
    )
    
    # Comprehensive evaluation
    evaluation_results = await evaluator.comprehensive_evaluation("data/validation_data.json")
    
    print(f"\nüìä Evaluation Results:")
    print(f"Average Teacher Reward: {evaluation_results['average_reward']:.3f}")
    print(f"Positive Response Rate: {evaluation_results['positive_rate']:.1%}")
    print(f"Excellent Response Rate: {evaluation_results['excellent_rate']:.1%}")
    
    # Save detailed results
    with open("evaluation_results.json", "w") as f:
        json.dump(evaluation_results, f, indent=2)

if __name__ == "__main__":
    asyncio.run(main())
```

<Tabs>
<Tab title="Model Upload Script">
```python
# src/upload_model.py
import os
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel
from huggingface_hub import HfApi, create_repo
from safetensors.torch import save_file
import json

class ModelUploader:
    def __init__(self, base_model_name: str, adapter_path: str, hf_token: str):
        self.base_model_name = base_model_name
        self.adapter_path = adapter_path
        self.hf_token = hf_token
        self.api = HfApi(token=hf_token)
    
    def merge_and_save(self, output_path: str):
        """Merge LoRA adapter with base model and save in SafeTensors format"""
        
        print("Loading base model...")
        base_model = AutoModelForCausalLM.from_pretrained(
            self.base_model_name,
            torch_dtype=torch.float16,
            device_map="auto"
        )
        
        print("Loading fine-tuned adapter...")
        model = PeftModel.from_pretrained(base_model, self.adapter_path)
        
        print("Merging adapter with base model...")
        merged_model = model.merge_and_unload()
        
        print("Loading tokenizer...")
        tokenizer = AutoTokenizer.from_pretrained(self.base_model_name)
        
        # Create output directory
        os.makedirs(output_path, exist_ok=True)
        
        print(f"Saving merged model to {output_path}...")
        
        # Save model in SafeTensors format
        merged_model.save_pretrained(
            output_path,
            safe_serialization=True,  # This enables SafeTensors format
            max_shard_size="2GB"
        )
        
        # Save tokenizer
        tokenizer.save_pretrained(output_path)
        
        # Create model card
        self.create_model_card(output_path)
        
        print("Model saved successfully in SafeTensors format!")
    
    def create_model_card(self, model_path: str):
        """Create a comprehensive model card"""
        
        model_card = f"""---
language: en
license: llama2
library_name: transformers
tags:
- llama
- fine-tuned
- instruction-following
- claude-generated-data
- safetensors
base_model: {self.base_model_name}
---

# Llama 8B Fine-tuned with Claude Sonnet 4 Data

This model is a fine-tuned version of [{self.base_model_name}](https://huggingface.co/{self.base_model_name}) using high-quality training data generated by Claude Sonnet 4 (claude-3-5-sonnet-20250514).

## Model Description

- **Base Model**: {self.base_model_name}
- **Fine-tuning Method**: LoRA (Low-Rank Adaptation)
- **Training Data**: Generated using Claude Sonnet 4
- **Format**: SafeTensors
- **Quantization**: 4-bit during training, full precision for inference

## Training Details

### Training Data
- Generated using Claude Sonnet 4 (claude-3-5-sonnet-20250514)
- Topics covered: machine learning, web development, databases, cloud architecture, cybersecurity, etc.
- Format: Instruction-response pairs
- Size: Approximately 50 high-quality examples

### Training Procedure
- **LoRA Configuration**:
  - Rank (r): 16
  - Alpha: 32
  - Dropout: 0.1
  - Target modules: q_proj, v_proj, k_proj, o_proj, gate_proj, up_proj, down_proj
- **Training Parameters**:
  - Epochs: 3
  - Batch size: 4 (with gradient accumulation)
  - Learning rate: 2e-4
  - Max sequence length: 2048

## Usage

```python
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch

# Load model and tokenizer
model_name = "your-username/llama-8b-claude-finetuned"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype=torch.float16,
    device_map="auto"
)

# Generate response
def generate_response(instruction):
    prompt = f'''### Instruction:
{{instruction}}

### Response:
'''
    
    inputs = tokenizer(prompt, return_tensors="pt")
    outputs = model.generate(**inputs, max_new_tokens=512, temperature=0.7)
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    
    return response.split("### Response:")[-1].strip()

# Example usage
response = generate_response("Explain machine learning algorithms with examples")
print(response)
```

## Performance

The model demonstrates improved instruction-following capabilities compared to the base model, particularly in:
- Technical explanations
- Code generation and debugging
- Problem-solving approaches
- Educational content creation

## Limitations

- Inherits limitations from the base Llama 2 8B model
- Training data size is relatively small (50 examples)
- May not perform as well as larger models on complex reasoning tasks

## Ethical Considerations

This model was trained on data generated by Claude Sonnet 4. Users should be aware of potential biases and limitations inherited from both the base model and the training data generation process.

## Citation

If you use this model, please cite:

```bibtex
@misc{{llama-8b-claude-finetuned,
  title={{Llama 8B Fine-tuned with Claude Sonnet 4 Data}},
  author={{Your Name}},
  year={{2025}},
  howpublished={{\\url{{https://huggingface.co/your-username/llama-8b-claude-finetuned}}}}
}}
```
"""
        
        with open(os.path.join(model_path, "README.md"), "w") as f:
            f.write(model_card)
    
    def upload_to_hf(self, local_path: str, repo_name: str, private: bool = False):
        """Upload model to Hugging Face Hub"""
        
        try:
            # Create repository
            print(f"Creating repository: {repo_name}")
            create_repo(
                repo_id=repo_name,
                token=self.hf_token,
                private=private,
                repo_type="model"
            )
            
            # Upload all files
            print("Uploading model files...")
            self.api.upload_folder(
                folder_path=local_path,
                repo_id=repo_name,
                token=self.hf_token,
                commit_message="Upload fine-tuned Llama 8B model with Claude Sonnet 4 data"
            )
            
            print(f"Model successfully uploaded to: https://huggingface.co/{repo_name}")
            
        except Exception as e:
            print(f"Error uploading model: {e}")

# Example usage
def main():
    uploader = ModelUploader(
        base_model_name="meta-llama/Llama-2-8b-chat-hf",
        adapter_path="models/llama-8b-finetuned",
        hf_token=os.getenv("HF_TOKEN")
    )
    
    # Merge and save model
    merged_path = "models/llama-8b-merged"
    uploader.merge_and_save(merged_path)
    
    # Upload to Hugging Face
    repo_name = "your-username/llama-8b-claude-finetuned"  # Change this
    uploader.upload_to_hf(merged_path, repo_name, private=False)

if __name__ == "__main__":
    main()
```
</Tab>

<Tab title="Verification Script">
```python
# verify_upload.py
from huggingface_hub import HfApi
import os

def verify_model_upload(repo_name: str, token: str):
    """Verify that the model was uploaded correctly"""
    
    api = HfApi(token=token)
    
    try:
        # Get repository info
        repo_info = api.repo_info(repo_id=repo_name, repo_type="model")
        
        print(f"Repository: {repo_name}")
        print(f"Created: {repo_info.created_at}")
        print(f"Last modified: {repo_info.last_modified}")
        print(f"Size: {repo_info.size}")
        
        # List files
        files = api.list_repo_files(repo_id=repo_name, repo_type="model")
        print(f"\nFiles in repository:")
        for file in files:
            print(f"  - {file}")
        
        # Check for SafeTensors files
        safetensors_files = [f for f in files if f.endswith('.safetensors')]
        if safetensors_files:
            print(f"\n‚úÖ SafeTensors files found: {len(safetensors_files)}")
        else:
            print("\n‚ùå No SafeTensors files found")
        
        return True
        
    except Exception as e:
        print(f"Error verifying upload: {e}")
        return False

if __name__ == "__main__":
    repo_name = "your-username/llama-8b-claude-finetuned"  # Change this
    token = os.getenv("HF_TOKEN")
    
    verify_model_upload(repo_name, token)
```
</Tab>
</Tabs>

## Step 5: Complete Workflow Script

```python
# main.py - Complete workflow
import asyncio
import os
from dotenv import load_dotenv
from src.dataset_preparation import DatasetPreparator
from src.finetuning import LlamaFineTuner
from src.evaluation import ModelEvaluator
from src.upload_model import ModelUploader
from src.claude_teacher import ClaudeTeacherAgent

async def main():
    """Complete fine-tuning workflow with Claude teacher agent"""
    
    load_dotenv()
    
    print("üöÄ Starting Llama 8B Fine-tuning with Claude Sonnet 4 Teacher Agent")
    
    # Step 1: Prepare dataset from Hugging Face
    print("\nüìä Step 1: Loading and preparing dataset from Hugging Face...")
    
    preparator = DatasetPreparator("HuggingFaceH4/ultrachat_200k")
    data = preparator.load_and_prepare_dataset(max_samples=1000)
    train_file, val_file = preparator.save_dataset(data)
    
    print(f"‚úÖ Dataset prepared: {data['total_samples']} samples")
    
    # Step 2: Initialize Claude teacher agent
    print("\nüßë‚Äçüè´ Step 2: Initializing Claude Sonnet 4 teacher agent...")
    
    teacher_agent = ClaudeTeacherAgent()
    
    # Test teacher agent
    test_instruction = "Explain the concept of machine learning"
    test_response = "Machine learning is a method where computers learn patterns."
    
    sample_evaluation = await teacher_agent.evaluate_response(test_instruction, test_response)
    print(f"‚úÖ Teacher agent initialized and tested")
    
    # Step 3: Fine-tune with teacher guidance
    print("\nüî• Step 3: Fine-tuning Llama 8B with teacher guidance...")
    
    fine_tuner = LlamaFineTuner()
    trainer = fine_tuner.train(
        train_file=train_file,
        val_file=val_file
    )
    
    print("‚úÖ Fine-tuning with teacher guidance completed!")
    
    # Step 4: Comprehensive evaluation
    print("\nüìà Step 4: Comprehensive evaluation with teacher agent...")
    
    evaluator = ModelEvaluator(
        base_model_name="meta-llama/Llama-2-8b-chat-hf",
        adapter_path="models/llama-8b-finetuned"
    )
    
    # Comprehensive evaluation
    evaluation_results = await evaluator.comprehensive_evaluation(val_file)
    
    print(f"üìä Final Results:")
    print(f"  Average Teacher Reward: {evaluation_results['average_reward']:.3f}")
    print(f"  Positive Response Rate: {evaluation_results['positive_rate']:.1%}")
    print(f"  Excellent Response Rate: {evaluation_results['excellent_rate']:.1%}")
    
    # Step 5: Compare with base model
    print("\nüîç Step 5: Comparing with base model...")
    
    test_prompts = [
        "Explain machine learning algorithms with examples",
        "How do you optimize database performance?",
        "What are the best practices for API design?",
        "Describe microservices architecture benefits",
        "How to implement secure authentication?"
    ]
    
    comparison_results = await evaluator.compare_with_base_model(
        "meta-llama/Llama-2-8b-chat-hf", 
        test_prompts
    )
    
    print(f"üéØ Improvement Results:")
    print(f"  Average Improvement: {comparison_results['average_improvement']:+.3f}")
    print(f"  Improvement Rate: {comparison_results['improvement_rate']:.1%}")
    
    # Step 6: Upload to Hugging Face
    print("\nüöÄ Step 6: Converting to SafeTensors and preparing for upload...")
    
    uploader = ModelUploader(
        base_model_name="meta-llama/Llama-2-8b-chat-hf",
        adapter_path="models/llama-8b-finetuned",
        hf_token=os.getenv("HF_TOKEN")
    )
    
    # Merge and save
    merged_path = "models/llama-8b-merged"
    uploader.merge_and_save(merged_path)
    
    # Create enhanced model card with teacher evaluation results
    create_enhanced_model_card(merged_path, evaluation_results, comparison_results)
    
    print("\nüéâ Fine-tuning workflow completed successfully!")
    print("\nüìù Next steps:")
    print("1. Review evaluation results in 'evaluation_results.json'")
    print("2. Update repo name in upload script and run upload")
    print("3. Test your uploaded model with the teacher agent")
    
    return {
        'evaluation_results': evaluation_results,
        'comparison_results': comparison_results,
        'model_path': merged_path
    }

def create_enhanced_model_card(model_path: str, eval_results: Dict, comparison_results: Dict):
    """Create enhanced model card with teacher evaluation results"""
    
    model_card = f"""---
language: en
license: llama2
library_name: transformers
tags:
- llama
- fine-tuned
- instruction-following
- claude-teacher-guided
- safetensors
- ultrachat
base_model: meta-llama/Llama-2-8b-chat-hf
---

# Llama 8B Fine-tuned with Claude Sonnet 4 Teacher Guidance

This model is a fine-tuned version of [meta-llama/Llama-2-8b-chat-hf](https://huggingface.co/meta-llama/Llama-2-8b-chat-hf) using Claude Sonnet 4 (claude-3-5-sonnet-20250514) as a teacher agent for guidance and evaluation.

## Model Description

- **Base Model**: meta-llama/Llama-2-8b-chat-hf
- **Fine-tuning Method**: LoRA (Low-Rank Adaptation) with teacher guidance
- **Teacher Agent**: Claude Sonnet 4 (claude-3-5-sonnet-20250514)
- **Training Data**: HuggingFace UltraChat 200k dataset
- **Format**: SafeTensors

## Teacher-Guided Training Results

### Performance Metrics (Claude Teacher Evaluation)
- **Average Teacher Reward**: {eval_results['average_reward']:.3f}/1.0
- **Positive Response Rate**: {eval_results['positive_rate']:.1%}
- **Excellent Response Rate**: {eval_results['excellent_rate']:.1%}

### Improvement over Base Model
- **Average Improvement**: {comparison_results['average_improvement']:+.3f}
- **Improvement Rate**: {comparison_results['improvement_rate']:.1%}

## Training Details

### Dataset
- **Source**: HuggingFaceH4/ultrachat_200k
- **Size**: 1,000 high-quality samples (800 train, 200 validation)
- **Format**: Instruction-response pairs

### Teacher Agent Integration
- **Real-time Evaluation**: Claude Sonnet 4 evaluated model responses during training
- **Reward Signals**: Teacher provided reward scores (-1.0 to 1.0) for training guidance
- **Continuous Guidance**: Improvement suggestions provided throughout training

### Training Configuration
- **LoRA Parameters**:
  - Rank (r): 16
  - Alpha: 32
  - Dropout: 0.1
  - Target modules: q_proj, v_proj, k_proj, o_proj, gate_proj, up_proj, down_proj
- **Training Parameters**:
  - Epochs: 3
  - Batch size: 4 (with gradient accumulation)
  - Learning rate: 2e-4
  - Max sequence length: 2048

## Usage

```python
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch

# Load model and tokenizer
model_name = "your-username/llama-8b-claude-teacher-guided"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype=torch.float16,
    device_map="auto"
)

# Generate response
def generate_response(instruction):
    prompt = f'''### Instruction:
{{instruction}}

### Response:
'''
    
    inputs = tokenizer(prompt, return_tensors="pt")
    outputs = model.generate(**inputs, max_new_tokens=512, temperature=0.7)
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    
    return response.split("### Response:")[-1].strip()

# Example usage
response = generate_response("Explain machine learning algorithms with examples")
print(response)
```

## Performance Highlights

This model demonstrates significant improvements over the base model, as validated by Claude Sonnet 4:

1. **Enhanced Instruction Following**: Better understanding and execution of complex instructions
2. **Improved Reasoning**: More logical and structured responses
3. **Quality Consistency**: Higher consistency in response quality across different topics
4. **Teacher Validation**: All improvements verified by state-of-the-art AI teacher

## Limitations

- Inherits limitations from the base Llama 2 8B model
- Training dataset size is moderate (1,000 samples)
- Teacher evaluation is based on Claude Sonnet 4's capabilities and potential biases

## Ethical Considerations

This model was trained with AI teacher guidance from Claude Sonnet 4. Users should be aware of potential biases inherited from both the base model, training data, and teacher model evaluations.

## Citation

```bibtex
@misc{{llama-8b-claude-teacher-guided,
  title={{Llama 8B Fine-tuned with Claude Sonnet 4 Teacher Guidance}},
  author={{Your Name}},
  year={{2025}},
  howpublished={{\\url{{https://huggingface.co/your-username/llama-8b-claude-teacher-guided}}}}
}}
```

## Acknowledgments

- Meta AI for the Llama 2 base model
- Anthropic for Claude Sonnet 4 teacher capabilities
- HuggingFace for the UltraChat dataset and infrastructure
"""
    
    with open(os.path.join(model_path, "README.md"), "w") as f:
        f.write(model_card)

if __name__ == "__main__":
    results = asyncio.run(main())
```
```

## Troubleshooting

<AccordionGroup>
<Accordion title="CUDA Out of Memory">
**Solutions:**
- Reduce batch size: `per_device_train_batch_size=2`
- Increase gradient accumulation: `gradient_accumulation_steps=8`
- Use 8-bit quantization: `load_in_8bit=True`
- Reduce sequence length: `max_length=1024`
</Accordion>

<Accordion title="Slow Training Speed">
**Optimizations:**
- Enable mixed precision: `fp16=True` or `bf16=True`
- Use gradient checkpointing: `gradient_checkpointing=True`
- Optimize DataLoader: `dataloader_num_workers=4`
- Use faster optimizer: `optim="adamw_torch_fused"`
</Accordion>

<Accordion title="Poor Model Performance">
**Improvements:**
- Increase training data quality and quantity
- Adjust learning rate: try `1e-4` to `5e-4`
- Increase LoRA rank: `r=32`
- Train for more epochs
- Use larger base model (13B, 70B)
</Accordion>

<Accordion title="Upload Failures">
**Solutions:**
- Check HF token permissions
- Verify SafeTensors conversion: `ls -la models/llama-8b-merged/*.safetensors`
- Use `git-lfs` for large files
- Upload in smaller chunks if needed
</Accordion>
</AccordionGroup>

## Best Practices

<CardGroup cols={2}>
<Card title="Data Quality" icon="chart-line">
  - Use diverse, high-quality prompts
  - Validate Claude-generated responses
  - Include various difficulty levels
  - Test on real-world scenarios
</Card>

<Card title="Training Efficiency" icon="rocket">
  - Monitor GPU memory usage
  - Use gradient accumulation for larger effective batch sizes
  - Implement early stopping
  - Save checkpoints regularly
</Card>

<Card title="Model Evaluation" icon="search">
  - Test on held-out validation set
  - Compare with base model performance
  - Evaluate on downstream tasks
  - Check for overfitting signs
</Card>

<Card title="Deployment" icon="upload">
  - Verify SafeTensors format
  - Test model loading after upload
  - Include comprehensive model card
  - Set appropriate repository visibility
</Card>
</CardGroup>

## Advanced Configuration

<Tabs>
<Tab title="Multi-GPU Training">
```python
# For multiple GPUs, use accelerate
from accelerate import Accelerator

accelerator = Accelerator()

# Modify training arguments
training_args = TrainingArguments(
    # ... other args ...
    ddp_find_unused_parameters=False,
    dataloader_pin_memory=False,
    ddp_backend="nccl",  # for NVIDIA GPUs
)

# Use accelerator in trainer
trainer = SFTTrainer(
    # ... other args ...
    args=training_args,
)

# Accelerate the training
model, optimizer, train_dataloader, lr_scheduler = accelerator.prepare(
    model, optimizer, train_dataloader, lr_scheduler
)
```
</Tab>

<Tab title="Custom LoRA Configuration">
```python
# Advanced LoRA config for better performance
lora_config = LoraConfig(
    r=64,  # Higher rank for more parameters
    lora_alpha=128,  # Scaled alpha
    target_modules=[
        "q_proj", "k_proj", "v_proj", "o_proj",
        "gate_proj", "up_proj", "down_proj",
        "lm_head"  # Include language model head
    ],
    lora_dropout=0.05,  # Lower dropout
    bias="none",
    task_type=TaskType.CAUSAL_LM,
    modules_to_save=["embed_tokens", "lm_head"]  # Save embeddings
)
```
</Tab>

<Tab title="Advanced Data Processing">
```python
# Enhanced data preprocessing
def advanced_formatting_func(example):
    """Advanced formatting with system prompts"""
    
    system_prompt = "You are a helpful AI assistant trained on high-quality data generated by Claude Sonnet 4."
    
    text = f"""<|system|>
{system_prompt}
<|user|>
{example['instruction']}
<|assistant|>
{example['output']}<|end|>"""
    
    return {"text": text}

# Apply advanced formatting
train_dataset = train_dataset.map(advanced_formatting_func)
```
</Tab>
</Tabs>

## Conclusion

This tutorial demonstrates a complete workflow for fine-tuning Meta Llama 8B Instruct using high-quality data generated by Claude Sonnet 4. The process includes:

1. **Data Generation**: Using Claude Sonnet 4 to create diverse, high-quality training examples
2. **Model Fine-tuning**: Efficiently fine-tuning using LoRA and 4-bit quantization
3. **Evaluation**: Testing the fine-tuned model's performance
4. **Deployment**: Converting to SafeTensors format and uploading to Hugging Face

The resulting model benefits from the high-quality reasoning and instruction-following capabilities demonstrated in Claude Sonnet 4's generated training data, while maintaining the efficiency of the Llama 8B architecture.

<Note>
Remember to comply with Meta's Llama 2 license terms and Anthropic's usage policies when using this approach for commercial applications.
</Note>