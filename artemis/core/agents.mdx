---
title: "Agents"
description: "Modern Agent API for Artemis RL with built-in RL Teacher capabilities"
---

The Artemis Agent API provides a unified interface for working with AI agents across different providers (OpenAI, Anthropic, Local models) and includes structured feedback systems for reinforcement learning training.

## Quick Start

```python
from artemis.agent import agent

# Create a basic agent
my_agent = agent().config(
    level="balanced",
    provider="anthropic",
    model_id="claude-3-sonnet-20240229"
)

await my_agent.initialize()
response = await my_agent.act("Hello, world!")
```

## Factory Pattern

### `agent()`

The main factory function for creating agent instances.

```python
from artemis.agent import agent

# Returns an AgentBuilder instance
builder = agent()
```

## Configuration

### `.config(**kwargs)`

Configure the agent with various parameters.

```python
agent_instance = agent().config(
    level="strict",                    # Grading/behavior level
    provider="anthropic",              # Provider: openai, anthropic, local
    model_id="claude-3-sonnet-20240229",  # Specific model
    temperature=0.7,                   # Sampling temperature
    max_tokens=1000,                   # Maximum response length
    top_p=0.9,                        # Nucleus sampling
    frequency_penalty=0.0,             # Repetition penalty
    presence_penalty=0.0,              # Presence penalty
    timeout=30.0,                      # Request timeout
    max_retries=3,                     # Retry attempts
    # Local model specific
    model=None,                        # PyTorch model instance
    tokenizer=None,                    # Tokenizer instance
    device="auto"                      # Device placement
)
```

### Level System

The level system controls the agent's behavior strictness and capabilities:

<CardGroup cols={2}>
<Card title="Lenient (1-3)" icon="leaf">
  Very forgiving, encourages experimentation
</Card>
<Card title="Balanced (4-6)" icon="scale-balanced">
  Standard evaluation, balanced feedback
</Card>
<Card title="Strict (7-8)" icon="gavel">
  High standards, detailed critiques
</Card>
<Card title="Harsh (9-10)" icon="fire">
  Expert-level expectations, rigorous evaluation
</Card>
<Card title="Beginner" icon="baby">
  Optimized for learning, patient feedback
</Card>
<Card title="Intermediate" icon="graduation-cap">
  Moderate expectations, constructive feedback
</Card>
<Card title="Expert" icon="trophy">
  Professional-grade evaluation
</Card>
</CardGroup>

### Provider-Specific Configuration

<Tabs>
<Tab title="OpenAI">
```python
openai_agent = agent().config(
    provider="openai",
    model_id="gpt-4",                  # gpt-4, gpt-3.5-turbo, etc.
    api_key="your-api-key",            # Optional if set in env
    organization="your-org-id"         # Optional
)
```
</Tab>

<Tab title="Anthropic">
```python
claude_agent = agent().config(
    provider="anthropic",
    model_id="claude-3-sonnet-20240229",  # claude-3-sonnet, claude-3-haiku
    api_key="your-api-key",            # Optional if set in env
    max_tokens=4000                    # Claude-specific max tokens
)
```
</Tab>

<Tab title="Local Models">
```python
local_agent = agent().config(
    provider="local",
    model=pytorch_model,               # Your PyTorch model
    tokenizer=tokenizer,               # HuggingFace tokenizer
    device="cuda:0",                   # Device placement
    generation_config={                # Generation parameters
        "max_new_tokens": 512,
        "temperature": 0.7,
        "do_sample": True
    }
)
```
</Tab>
</Tabs>

## Core Methods

<AccordionGroup>
<Accordion title=".initialize()">
Initialize the agent and establish connections.

```python
await agent_instance.initialize()
```
</Accordion>

<Accordion title=".act(prompt, **kwargs)">
Generate a response to a prompt.

```python
response = await agent.act("Write a Python function to sort a list")

# Response format:
{
    "text": "def sort_list(lst):\n    return sorted(lst)",
    "metadata": {
        "model": "claude-3-sonnet-20240229",
        "provider": "anthropic",
        "timestamp": "2025-09-15T10:30:00Z",
        "tokens_used": 45
    }
}
```

**Parameters:**
- `prompt` (str): The input prompt
- `system_prompt` (str, optional): System-level instructions
- `context` (List[Dict], optional): Conversation history
- `tools` (List[Dict], optional): Available tools/functions
</Accordion>
</AccordionGroup>

## RL Teacher Model Functions

<AccordionGroup>
<Accordion title=".grade(student_response, reference_answer=None, prompt=None, **kwargs)">
Provide structured grading for a student response.

```python
grading = await teacher.grade(
    student_response="def reverse_string(s): return s[::-1]",
    reference_answer="Standard slicing approach",
    prompt="Write a function to reverse a string"
)

# Response format:
{
    "score": 85,
    "feedback": "Good implementation using slicing. Clean and efficient.",
    "breakdown": {
        "correctness": 90,
        "efficiency": 85,
        "readability": 80,
        "style": 75,
        "documentation": 60
    },
    "strengths": [
        "Correct algorithm",
        "Clean syntax",
        "Efficient approach"
    ],
    "weaknesses": [
        "Missing docstring",
        "No input validation"
    ],
    "suggestions": [
        "Add type hints",
        "Include error handling",
        "Add documentation"
    ],
    "confidence": 0.92
}
```

<Info>
**Grading Components:**
- **Correctness**: Algorithm accuracy and edge case handling
- **Efficiency**: Time and space complexity
- **Readability**: Code clarity and structure
- **Style**: Following language conventions
- **Documentation**: Comments and docstrings
</Info>
</Accordion>

<Accordion title=".reward(student_response, reference_answer=None, prompt=None, **kwargs)">
Generate reward signals for reinforcement learning.

```python
reward = await teacher.reward(
    student_response="def factorial(n): return 1 if n <= 1 else n * factorial(n-1)",
    prompt="Write a factorial function"
)

# Response format:
{
    "reward": 0.75,
    "confidence": 0.88,
    "reasoning": "Correct recursive implementation with base case",
    "components": {
        "correctness_reward": 0.9,
        "efficiency_reward": 0.6,
        "style_reward": 0.8
    },
    "feedback": "Good recursive solution, consider iterative for efficiency"
}
```

<Warning>
**Reward Range:** -1.0 to +1.0
- **Positive rewards**: Good performance, correct solutions
- **Negative rewards**: Errors, poor practices, inefficient code
- **Zero reward**: Neutral or baseline performance
</Warning>
</Accordion>

<Accordion title=".prefer(response_a, response_b, prompt=None, **kwargs)">
Compare two responses and determine preference.

```python
preference = await teacher.prefer(
    response_a="def add(a, b): return a + b",
    response_b="def add(x, y):\n    \"\"\"Add two numbers.\"\"\"\n    return x + y",
    prompt="Write an addition function"
)

# Response format:
{
    "preferred": "B",
    "confidence": 0.85,
    "margin": 0.65,
    "reasoning": "Response B includes documentation and better variable names",
    "comparison": {
        "correctness": {"A": 1.0, "B": 1.0},
        "documentation": {"A": 0.0, "B": 1.0},
        "style": {"A": 0.7, "B": 0.9}
    },
    "justification": "Both are correct, but B follows better practices"
}
```

<Tip>
**Preference Values:**
- **"A"**: First response is preferred
- **"B"**: Second response is preferred
- **"TIE"**: Responses are equivalent
</Tip>
</Accordion>
</AccordionGroup>

## Advanced Features

<Tabs>
<Tab title="Context Management">
```python
# Maintain conversation context
context = []

response1 = await agent.act("What is Python?", context=context)
context.append({"role": "user", "content": "What is Python?"})
context.append({"role": "assistant", "content": response1["text"]})

response2 = await agent.act("Give me an example", context=context)
```
</Tab>

<Tab title="Tool Integration">
```python
tools = [
    {
        "name": "calculator",
        "description": "Perform mathematical calculations",
        "parameters": {
            "type": "object",
            "properties": {
                "expression": {"type": "string"}
            }
        }
    }
]

response = await agent.act(
    "Calculate 15 * 24",
    tools=tools
)
```
</Tab>

<Tab title="Batch Processing">
```python
# Grade multiple responses
responses = ["solution1", "solution2", "solution3"]
prompts = ["problem1", "problem2", "problem3"]

grades = await asyncio.gather(*[
    teacher.grade(resp, prompt=prompt)
    for resp, prompt in zip(responses, prompts)
])
```
</Tab>

<Tab title="Error Handling">
```python
try:
    response = await agent.act("Complex prompt")
except AgentInitializationError:
    print("Agent not properly initialized")
except APIError as e:
    print(f"API error: {e}")
except TimeoutError:
    print("Request timed out")
except Exception as e:
    print(f"Unexpected error: {e}")
```
</Tab>
</Tabs>

## Training Integration

### GRPO Training Example

```python
from artemis.rl_algorithms.grpo import GRPOTrainer, GRPOConfig

# Setup teacher and student
teacher = agent().config(level="strict", provider="anthropic")
student = agent().config(level="beginner", provider="local", model=llama_model)

await teacher.initialize()
await student.initialize()

# Collect training data
training_data = []
for prompt in coding_problems:
    student_response = await student.act(prompt)
    teacher_grade = await teacher.grade(student_response["text"], prompt=prompt)
    teacher_reward = await teacher.reward(student_response["text"], prompt=prompt)
    
    training_data.append({
        "prompt": prompt,
        "response": student_response["text"],
        "reward": teacher_reward["reward"],
        "grade": teacher_grade["score"]
    })

# Setup GRPO trainer
gpro_config = GRPOConfig(learning_rate=1e-5, batch_size=4)
trainer = GRPOTrainer(config=gpro_config, model=llama_model, tokenizer=tokenizer)

# Train with preference pairs
preference_pairs = create_preference_pairs(training_data)
results = await trainer.train(preference_pairs)
```

## BaseAgent Class Interface

For custom implementations, the `BaseAgent` class provides the foundation:

### Core Methods

```python
from artemis.core.agent import BaseAgent, AgentConfig

class BaseAgent(ABC):
    @abstractmethod
    def get_action(self, observation: Observation) -> Action:
        """Generate an action based on the current observation."""
        pass
    
    @abstractmethod
    def update_policy(self, experience: Experience) -> None:
        """Update the agent's policy using collected experience."""
        pass
    
    def evaluate(self, environment: BaseEnvironment, num_episodes: int = 10) -> Dict[str, float]:
        """Evaluate agent performance."""
        pass
    
    def save_checkpoint(self, path: str) -> None:
        """Save agent state to checkpoint file."""
        pass
```

### Custom Agent Example

```python
class CustomAgent(BaseAgent):
    def __init__(self, config: AgentConfig):
        super().__init__(config)
        # Custom initialization
        
    def get_action(self, observation):
        # Custom action selection logic
        return self.policy(observation)
```

## Best Practices

<CardGroup cols={2}>
<Card title="Level Selection" icon="target">
  - Use **"strict"** for training high-quality models
  - Use **"balanced"** for general evaluation  
  - Use **"lenient"** for encouraging experimentation
</Card>

<Card title="Provider Choice" icon="server">
  - **Anthropic Claude**: Excellent for code review and detailed feedback
  - **OpenAI GPT-4**: Strong general capabilities and reasoning
  - **Local models**: Full control and privacy
</Card>

<Card title="Structured Feedback" icon="chart-line">
  - Always check `confidence` scores in responses
  - Use `breakdown` components for detailed analysis
  - Leverage `suggestions` for improvement guidance
</Card>

<Card title="Performance Optimization" icon="rocket">
  - Use batch processing for multiple evaluations
  - Set appropriate `timeout` values
  - Configure `max_retries` for reliability
</Card>

<Card title="Error Handling" icon="shield-check">
  - Always wrap API calls in try-catch blocks
  - Handle rate limiting gracefully
  - Implement fallback strategies
</Card>
</CardGroup>

## Examples

<Tabs>
<Tab title="Basic Usage">
```python
# Create and use an agent
chatbot = agent().config(level="balanced", provider="openai")
await chatbot.initialize()
response = await chatbot.act("Explain quantum computing")
print(response["text"])
```
</Tab>

<Tab title="Code Evaluation">
```python
# Setup code reviewer
reviewer = agent().config(level="strict", provider="anthropic")
await reviewer.initialize()

code = "def bubble_sort(arr): ..."
grade = await reviewer.grade(code, prompt="Implement bubble sort")
print(f"Score: {grade['score']}/100")
print(f"Feedback: {grade['feedback']}")
```
</Tab>

<Tab title="Preference Learning">
```python
# Compare two implementations
judge = agent().config(level="balanced", provider="anthropic")
await judge.initialize()

solution_a = "Quick implementation"
solution_b = "Detailed implementation with docs"

preference = await judge.prefer(solution_a, solution_b, prompt="Code quality")
print(f"Preferred: {preference['preferred']}")
print(f"Reasoning: {preference['reasoning']}")
```
</Tab>
</Tabs>

## Migration from V1

<CodeGroup>
```python Old API (V1)
from artemis.core.agent import BaseAgent, AgentConfig, AgentType

config = AgentConfig(
    name="my_agent",
    agent_type=AgentType.LLM_BASED,
    model_name="gpt-4"
)
agent = BaseAgent(config)
```

```python New API (V2)
from artemis.agent import agent

my_agent = agent().config(
    provider="openai",
    model_id="gpt-4"
)
```
</CodeGroup>

<Note>
**Key Changes:**
1. **Factory pattern**: `agent()` instead of direct instantiation
2. **Simplified config**: Direct parameters instead of config objects
3. **Level system**: Replaces mode-based API
4. **RL functions**: Built-in `grade()`, `reward()`, `prefer()` methods
5. **Provider abstraction**: Unified interface across providers
</Note>