---
title: "Agents"
description: "Understanding and working with Artemis RL agents"
---

Agents are the core decision-making entities in Artemis RL Gym. They observe environments, take actions, and learn from experience to improve their performance over time. This guide covers the different types of agents available and how to work with them effectively.

## Agent Types

<Tabs>
<Tab title="BaseAgent">
  The foundational interface that all agents implement. Provides standardized methods for action selection, policy updates, and state management.

  ```python
  from artemis.core.agent import BaseAgent, AgentConfig

  @dataclass
  class AgentConfig:
      agent_type: AgentType
      model_name: str
      max_tokens: int = 512
      temperature: float = 0.7
      top_p: float = 0.9
      device: str = "cuda"
      batch_size: int = 1
      use_distributed: bool = False
  ```
</Tab>

<Tab title="LLMAgent">
  Specialized agent for Large Language Model training with built-in optimizations for text generation tasks.

  ```python
  from artemis.agents.llm_agent import LLMAgent, LLMAgentConfig

  @dataclass
  class LLMAgentConfig(AgentConfig):
      model_name: str = "microsoft/DialoGPT-medium"
      model_type: str = "auto"
      use_lora: bool = True
      lora_r: int = 16
      lora_alpha: int = 32
      max_new_tokens: int = 512
      use_grpo: bool = True
      grpo_beta: float = 0.1
  ```
</Tab>

<Tab title="Custom Agents">
  Build specialized agents for specific domains by extending the BaseAgent class.

  ```python
  class CustomAgent(BaseAgent):
      def __init__(self, config: AgentConfig):
          super().__init__(config)
          # Custom initialization
          
      def get_action(self, observation):
          # Custom action selection logic
          return self.policy(observation)
  ```
</Tab>
</Tabs>

## BaseAgent Interface

The `BaseAgent` class provides the foundation for all agents in Artemis:

### Core Methods

<AccordionGroup>
<Accordion title="get_action(observation) → Action">
  Primary method for action selection. Takes an observation and returns an action.

  ```python
  class BaseAgent(ABC):
      @abstractmethod
      def get_action(self, observation: Observation) -> Action:
          """
          Generate an action based on the current observation.
          
          Args:
              observation: Current environment state
              
          Returns:
              Action to take in the environment
          """
          pass
  ```
</Accordion>

<Accordion title="update_policy(experience) → None">
  Updates the agent's policy based on collected experience.

  ```python
  @abstractmethod
  def update_policy(self, experience: Experience) -> None:
      """
      Update the agent's policy using collected experience.
      
      Args:
          experience: Tuple of (observation, action, reward, next_observation)
      """
      pass
  ```
</Accordion>

<Accordion title="evaluate(environment) → Dict">
  Evaluates the agent's performance in a given environment.

  ```python
  def evaluate(self, environment: BaseEnvironment, num_episodes: int = 10) -> Dict[str, float]:
      """
      Evaluate agent performance.
      
      Returns:
          Dictionary containing evaluation metrics
      """
      pass
  ```
</Accordion>

<Accordion title="save_checkpoint(path) → None">
  Saves the agent's current state for later restoration.

  ```python
  def save_checkpoint(self, path: str) -> None:
      """Save agent state to checkpoint file"""
      pass
  ```
</Accordion>
</AccordionGroup>

### Additional Methods

```python
class BaseAgent:
    def get_action_probabilities(self, observation: Observation) -> torch.Tensor:
        """Get probability distribution over actions"""
        
    def set_training_mode(self, training: bool) -> None:
        """Switch between training and evaluation modes"""
        
    def get_training_metrics(self) -> Dict[str, float]:
        """Return current training metrics"""
        
    def reset_episode(self) -> None:
        """Reset agent state for new episode"""
```

## LLMAgent - Large Language Model Agent

The `LLMAgent` is optimized for training Large Language Models using reinforcement learning:

### Key Features

<CardGroup cols={2}>
<Card title="Universal Model Support" icon="brain">
  Works with 50+ open-source models from Hugging Face with automatic model type detection
</Card>

<Card title="LoRA Integration" icon="zap">
  Built-in Low-Rank Adaptation for memory-efficient fine-tuning
</Card>

<Card title="GRPO Training" icon="target">
  Native Group Relative Policy Optimization for improved learning
</Card>

<Card title="Distributed Ready" icon="network-wired">
  Seamless integration with Artemis distributed training system
</Card>
</CardGroup>

### Configuration Options

```python
from artemis.agents.llm_agent import LLMAgent, LLMAgentConfig

config = LLMAgentConfig(
    # Model selection
    model_name="meta-llama/Llama-3.1-8B-Instruct",
    model_type="auto",  # Automatic detection
    
    # Memory optimization
    use_lora=True,
    lora_r=16,
    lora_alpha=32,
    lora_dropout=0.1,
    
    # Generation parameters
    max_new_tokens=512,
    temperature=0.7,
    top_p=0.9,
    do_sample=True,
    
    # Training optimization
    use_grpo=True,
    grpo_beta=0.1,
    use_gradient_checkpointing=True,
    use_fp16=True,
    
    # Device and batch settings
    device="cuda",
    batch_size=4,
    gradient_accumulation_steps=2
)

agent = LLMAgent(config)
```

### Supported Model Families

The LLMAgent automatically detects and optimizes for different model types:

<Tabs>
<Tab title="Chat Models">
  Models specifically designed for conversational AI:

  ```python
  # Popular chat models
  "meta-llama/Llama-3.1-8B-Instruct"
  "mistralai/Mistral-7B-Instruct-v0.1"
  "openchat/openchat-3.5-0106"
  "HuggingFaceH4/zephyr-7b-beta"
  "berkeley-nest/Starling-LM-7B-alpha"
  
  # Optimized for dialogue and instruction following
  agent = LLMAgent(LLMAgentConfig(
      model_name="openchat/openchat-3.5-0106",
      temperature=0.7,  # Balanced creativity
      max_new_tokens=256
  ))
  ```
</Tab>

<Tab title="Code Models">
  Specialized for code generation and programming tasks:

  ```python
  # Code generation models
  "codellama/CodeLlama-7b-Instruct-hf"
  "deepseek-ai/deepseek-coder-6.7b-instruct"
  "bigcode/starcoder"
  "ise-uiuc/Magicoder-S-DS-6.7B"
  
  # Optimized for precise code generation
  agent = LLMAgent(LLMAgentConfig(
      model_name="deepseek-ai/deepseek-coder-6.7b-instruct",
      temperature=0.1,  # Low temperature for precision
      max_new_tokens=1024  # Longer sequences for code
  ))
  ```
</Tab>

<Tab title="Math Models">
  Specialized for mathematical reasoning and problem solving:

  ```python
  # Mathematical reasoning models
  "WizardLM/WizardMath-7B-V1.1"
  "meta-math/MetaMath-7B-V1.0"
  "mistralai/Mathstral-7B-v0.1"
  
  # Optimized for step-by-step reasoning
  agent = LLMAgent(LLMAgentConfig(
      model_name="WizardLM/WizardMath-7B-V1.1",
      temperature=0.3,  # Moderate creativity for problem solving
      use_grpo=True,    # Enhanced learning for math tasks
      grpo_beta=0.05    # Lower beta for stability
  ))
  ```
</Tab>

<Tab title="Small Models">
  Lightweight models for development and testing:

  ```python
  # Small efficient models
  "TinyLlama/TinyLlama-1.1B-Chat-v1.0"
  "microsoft/DialoGPT-medium"
  "openbmb/MiniCPM-2B-sft-bf16"
  
  # Fast training and inference
  agent = LLMAgent(LLMAgentConfig(
      model_name="TinyLlama/TinyLlama-1.1B-Chat-v1.0",
      batch_size=8,     # Larger batches for small models
      use_lora=False,   # Full fine-tuning for small models
      use_fp16=True     # Memory optimization
  ))
  ```
</Tab>
</Tabs>

## Usage Examples

### Basic Agent Training

```python
import asyncio
from artemis.agents.llm_agent import LLMAgent, LLMAgentConfig
from artemis.environments.conversation_env import ConversationEnvironment

async def basic_agent_training():
    # Configure agent
    config = LLMAgentConfig(
        model_name="microsoft/DialoGPT-medium",
        use_lora=True,
        max_new_tokens=256
    )
    
    agent = LLMAgent(config)
    environment = ConversationEnvironment()
    
    # Training loop
    for episode in range(100):
        observation = environment.reset()
        total_reward = 0
        
        while not environment.done:
            action = agent.get_action(observation)
            next_obs, reward, done = environment.step(action)
            
            # Collect experience
            experience = (observation, action, reward, next_obs)
            agent.update_policy(experience)
            
            observation = next_obs
            total_reward += reward
        
        print(f"Episode {episode}: Reward = {total_reward:.3f}")
    
    # Save trained agent
    agent.save_checkpoint("trained_agent.pt")

asyncio.run(basic_agent_training())
```

### Multi-Agent Training

```python
from artemis.agents.llm_agent import LLMAgent
from artemis.distributed.trainer_hub import DistributedTrainerHub

async def multi_agent_training():
    # Create multiple agents with different configurations
    agents = []
    
    configs = [
        {"model_name": "TinyLlama/TinyLlama-1.1B-Chat-v1.0", "temperature": 0.5},
        {"model_name": "microsoft/DialoGPT-medium", "temperature": 0.7},
        {"model_name": "openchat/openchat-3.5-0106", "temperature": 0.9}
    ]
    
    for i, config in enumerate(configs):
        agent_config = LLMAgentConfig(**config)
        agent = LLMAgent(agent_config)
        agents.append(agent)
    
    # Distributed training
    trainer_hub = DistributedTrainerHub({
        "agents": agents,
        "environment": "conversation",
        "training_method": "competitive"  # Agents compete against each other
    })
    
    await trainer_hub.train(episodes=500)
    
    # Evaluate and select best agent
    best_agent = trainer_hub.get_best_agent()
    best_agent.save_checkpoint("best_multi_agent.pt")

asyncio.run(multi_agent_training())
```

### Custom Agent Implementation

```python
from artemis.core.agent import BaseAgent, AgentConfig
import torch
import torch.nn as nn

class BusinessReasoningAgent(BaseAgent):
    """Custom agent for business reasoning tasks"""
    
    def __init__(self, config: AgentConfig):
        super().__init__(config)
        
        # Custom components
        self.domain_classifier = nn.Linear(768, 5)  # 5 business domains
        self.risk_assessor = nn.Linear(768, 1)      # Risk score
        self.strategy_generator = nn.Linear(768, 512) # Strategy embeddings
        
        # Business domain knowledge
        self.domains = ["finance", "marketing", "operations", "strategy", "hr"]
        self.risk_thresholds = {"low": 0.3, "medium": 0.6, "high": 0.9}
    
    def get_action(self, observation):
        """Generate business-focused response"""
        
        # Extract business context
        domain_probs = self.domain_classifier(observation.embedding)
        domain = self.domains[torch.argmax(domain_probs)]
        
        # Assess risk
        risk_score = torch.sigmoid(self.risk_assessor(observation.embedding))
        risk_level = self._categorize_risk(risk_score)
        
        # Generate domain-specific strategy
        strategy_embedding = self.strategy_generator(observation.embedding)
        response = self._generate_business_response(
            domain, risk_level, strategy_embedding
        )
        
        return response
    
    def _categorize_risk(self, score):
        for level, threshold in self.risk_thresholds.items():
            if score < threshold:
                return level
        return "high"
    
    def _generate_business_response(self, domain, risk, strategy_emb):
        # Domain-specific response generation
        prompt_template = self._get_domain_template(domain, risk)
        return self.language_model.generate(
            prompt_template,
            context_embedding=strategy_emb
        )

# Usage
config = AgentConfig(
    agent_type=AgentType.CUSTOM,
    model_name="meta-llama/Llama-3.1-8B-Instruct"
)

business_agent = BusinessReasoningAgent(config)
```

## Advanced Features

### Memory Management

<Tabs>
<Tab title="LoRA Training">
  Low-Rank Adaptation for memory-efficient training:

  ```python
  config = LLMAgentConfig(
      model_name="meta-llama/Llama-3.1-8B-Instruct",
      use_lora=True,
      lora_r=16,        # Rank of adaptation
      lora_alpha=32,    # Scaling factor
      lora_dropout=0.1, # Regularization
      target_modules=["q_proj", "v_proj", "k_proj", "o_proj"]
  )
  ```

  Benefits:
  - 90% reduction in trainable parameters
  - Faster training and inference
  - Lower memory requirements
  - Easy model switching
</Tab>

<Tab title="Gradient Checkpointing">
  Trade computation for memory:

  ```python
  config = LLMAgentConfig(
      use_gradient_checkpointing=True,
      gradient_accumulation_steps=4,  # Effective batch size = 4 * batch_size
      max_grad_norm=1.0              # Gradient clipping
  )
  ```

  Benefits:
  - Reduced memory usage during backpropagation
  - Enable larger models on smaller GPUs
  - Stable training with gradient clipping
</Tab>

<Tab title="Mixed Precision">
  Automatic mixed precision training:

  ```python
  config = LLMAgentConfig(
      use_fp16=True,           # Half precision
      fp16_opt_level="O1",     # Conservative mixed precision
      loss_scale=128.0         # Loss scaling for stability
  )
  ```

  Benefits:
  - 50% memory reduction
  - Faster training on modern GPUs
  - Maintained numerical stability
</Tab>
</Tabs>

### Performance Optimization

<AccordionGroup>
<Accordion title="Batch Processing">
  Optimize throughput with intelligent batching:

  ```python
  config = LLMAgentConfig(
      batch_size=4,
      max_sequence_length=512,
      pad_to_multiple_of=8,    # Efficient tensor operations
      dataloader_num_workers=4, # Parallel data loading
      pin_memory=True          # Faster GPU transfers
  )
  ```
</Accordion>

<Accordion title="Model Compilation">
  Use PyTorch compilation for faster inference:

  ```python
  config = LLMAgentConfig(
      compile_model=True,
      compile_mode="reduce-overhead",  # Optimize for inference
      dynamic_shapes=False            # Static shapes for compilation
  )
  ```
</Accordion>

<Accordion title="KV-Cache Optimization">
  Optimize attention mechanisms:

  ```python
  config = LLMAgentConfig(
      use_kv_cache=True,
      kv_cache_dtype="fp16",
      max_cache_length=2048,
      sliding_window_size=1024  # For very long sequences
  )
  ```
</Accordion>
</AccordionGroup>

## Debugging and Monitoring

### Debug Mode

Enable comprehensive debugging:

```python
config = LLMAgentConfig(
    debug_mode=True,
    log_level="DEBUG",
    save_intermediate_outputs=True,
    track_gradients=True
)

agent = LLMAgent(config)

# Access debug information
debug_info = agent.get_debug_info()
print(f"Model parameters: {debug_info['num_parameters']}")
print(f"Memory usage: {debug_info['memory_usage_mb']} MB")
print(f"Last forward pass time: {debug_info['last_forward_time_ms']} ms")
```

### Training Metrics

Monitor training progress:

```python
# Get real-time metrics
metrics = agent.get_training_metrics()
print(f"Loss: {metrics['loss']:.4f}")
print(f"Learning rate: {metrics['lr']:.6f}")
print(f"Gradient norm: {metrics['grad_norm']:.4f}")

# Set up metric logging
from artemis.utils.logging import MetricLogger

logger = MetricLogger("agent_training.log")
agent.set_metric_logger(logger)

# Metrics are automatically logged during training
```

## Best Practices

<Tip>
**Model Selection**: Start with smaller models for development (`TinyLlama`, `DialoGPT-medium`) and scale up to production models (`Llama-3.1-8B`, `Mistral-7B`) once your pipeline is working.
</Tip>

<Warning>
**Memory Management**: Always enable LoRA for models larger than 3B parameters unless you have abundant GPU memory. Monitor memory usage and adjust batch sizes accordingly.
</Warning>

<Note>
**Temperature Settings**: Use lower temperatures (0.1-0.3) for tasks requiring precision (code, math), moderate temperatures (0.5-0.7) for balanced tasks (conversation), and higher temperatures (0.8-1.0) for creative tasks.
</Note>

## Next Steps

<CardGroup cols={2}>
<Card title="Environments" icon="globe" href="/artemis/core/environments">
  Learn about the environments where agents train and operate
</Card>

<Card title="RL Algorithms" icon="calculator" href="/artemis/algorithms/overview">
  Dive into the learning algorithms that power agent improvement
</Card>
</CardGroup>