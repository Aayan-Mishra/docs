---
title: "Quickstart Guide"
description: "Get started with Artemis RL Gym in under 10 minutes"
---

Get up and running with Artemis RL Gym in minutes. This guide will walk you through installation, basic setup, and your first training run.

## Prerequisites

Before you begin, ensure you have:

- Python 3.8 or higher
- CUDA-compatible GPU (recommended)
- At least 8GB of RAM
- Basic familiarity with reinforcement learning concepts

<Note>
While a GPU is recommended for optimal performance, Artemis can run on CPU-only systems for development and testing.
</Note>

## Installation

<Steps>
<Step title="Install Artemis RL Gym">
  Install the package using pip:

  ```bash
  pip install artemis-rl-gym
  ```

  For development installations with additional tools:

  ```bash
  pip install artemis-rl-gym[dev]
  ```
</Step>

<Step title="Verify Installation">
  Test your installation:

  ```python
  import artemis
  print(f"Artemis version: {artemis.__version__}")
  ```

  <Check>
  You should see the version number printed without any errors.
  </Check>
</Step>

<Step title="Optional: GPU Setup">
  If you have a CUDA-compatible GPU, install PyTorch with CUDA support:

  ```bash
  pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
  ```

  Verify GPU availability:

  ```python
  import torch
  print(f"CUDA available: {torch.cuda.is_available()}")
  print(f"GPU count: {torch.cuda.device_count()}")
  ```
</Step>
</Steps>

## Your First Training Run

Let's start with a simple conversation training example using a small model:

<CodeGroup>
```python Basic Training
import asyncio
from artemis import train_llm_grpo

async def basic_training():
    """Simple conversation training with DialoGPT"""
    
    results = await train_llm_grpo(
        model_name="microsoft/DialoGPT-medium",
        env_type="conversation",
        num_episodes=50,
        distributed=False,  # Single node for simplicity
        output_dir="./artemis_results"
    )
    
    print(f"Training completed!")
    print(f"Final reward: {results['final_reward']:.3f}")
    print(f"Episodes completed: {results['episodes_completed']}")

# Run the training
asyncio.run(basic_training())
```

```python Advanced Training
import asyncio
from artemis import train_llm_grpo
from artemis.rl_algorithms.grpo import GRPOConfig

async def advanced_training():
    """Advanced training with custom configuration"""
    
    # Configure GRPO algorithm
    grpo_config = GRPOConfig(
        learning_rate=1e-4,
        num_reference_policies=3,
        reference_weight=0.1,
        kl_penalty_coefficient=0.02,
        clip_epsilon=0.2
    )
    
    results = await train_llm_grpo(
        model_name="meta-llama/Llama-3.1-8B-Instruct",
        env_type="math",
        num_episodes=200,
        algorithm_config=grpo_config,
        distributed=True,  # Enable distributed training
        monitoring=True,   # Enable real-time monitoring
        output_dir="./artemis_advanced"
    )
    
    print(f"Advanced training completed!")
    print(f"Final reward: {results['final_reward']:.3f}")

asyncio.run(advanced_training())
```

```python Multi-Environment
import asyncio
from artemis.distributed import DeploymentManager

async def multi_environment_training():
    """Train across multiple specialized environments"""
    
    config = {
        "model_name": "openchat/openchat-3.5-0106",
        "environments": ["math", "code", "conversation"],
        "episodes_per_env": 100,
        "distributed": True,
        "auto_scaling": {
            "min_instances": 2,
            "max_instances": 8,
            "target_cpu": 70
        }
    }
    
    deployment = DeploymentManager(config)
    await deployment.start()
    
    # Training runs automatically across environments
    # Monitor at http://localhost:8000
    
    print("Multi-environment training started!")
    print("Monitor progress at http://localhost:8000")

asyncio.run(multi_environment_training())
```
</CodeGroup>

## Understanding the Output

When training completes, you'll see output similar to:

```
✅ Artemis Training Results
├── Model: microsoft/DialoGPT-medium
├── Environment: conversation
├── Episodes: 50/50
├── Final Reward: 0.847
├── Training Time: 12m 34s
├── Average Response Time: 234ms
└── Checkpoints: ./artemis_results/checkpoints/
```

<Tip>
Check the `output_dir` for saved model checkpoints, training logs, and performance metrics.
</Tip>

## Monitoring Dashboard

If you enabled monitoring (`monitoring=True`), access the real-time dashboard:

1. Open your browser to `http://localhost:8000`
2. View training progress, system metrics, and performance analytics
3. Monitor distributed workers and auto-scaling events

<Frame>
  <img src="/images/artemis-dashboard.png" alt="Artemis Monitoring Dashboard" />
</Frame>

## Common Configuration Options

<Tabs>
<Tab title="Model Selection">
  ```python
  # Small models for development
  model_name="TinyLlama/TinyLlama-1.1B-Chat-v1.0"
  model_name="microsoft/DialoGPT-medium"
  
  # Production models
  model_name="meta-llama/Llama-3.1-8B-Instruct"
  model_name="mistralai/Mistral-7B-Instruct-v0.1"
  model_name="openchat/openchat-3.5-0106"
  
  # Specialized models
  model_name="WizardLM/WizardMath-7B-V1.1"  # Math reasoning
  model_name="deepseek-ai/deepseek-coder-6.7b-instruct"  # Code
  ```
</Tab>

<Tab title="Environment Types">
  ```python
  env_type="conversation"  # Dialogue and chat
  env_type="math"         # Mathematical reasoning
  env_type="code"         # Code generation
  env_type="reasoning"    # General reasoning
  env_type="gsm8k"       # Grade school math dataset
  env_type="mmlu"        # Multitask language understanding
  env_type="human_eval"  # Code evaluation dataset
  ```
</Tab>

<Tab title="Training Scale">
  ```python
  # Development (single node)
  distributed=False
  num_episodes=50
  
  # Small scale production
  distributed=True
  num_episodes=500
  
  # Large scale production
  distributed=True
  num_episodes=5000
  auto_scaling={"min_instances": 4, "max_instances": 16}
  ```
</Tab>
</Tabs>

## Next Steps

Now that you have Artemis running, explore these areas:

<CardGroup cols={2}>
<Card title="Core Components" icon="cube" href="/artemis/core/overview">
  Learn about agents, environments, and algorithms
</Card>

<Card title="RL Algorithms" icon="brain" href="/artemis/algorithms/overview">
  Dive deep into GRPO, DPO, and PPO implementations
</Card>

<Card title="Distributed Training" icon="network-wired" href="/artemis/distributed/overview">
  Scale your training across multiple nodes
</Card>

<Card title="Examples" icon="code" href="/artemis/examples">
  Explore practical usage examples and tutorials
</Card>
</CardGroup>

## Troubleshooting

<AccordionGroup>
<Accordion title="CUDA out of memory">
  **Problem**: GPU memory errors during training.
  
  **Solutions**:
  - Reduce batch size: `batch_size=1`
  - Use gradient checkpointing: `use_gradient_checkpointing=True`
  - Enable LoRA: `use_lora=True`
  - Use a smaller model for testing
</Accordion>

<Accordion title="Model not found">
  **Problem**: Hugging Face model cannot be loaded.
  
  **Solutions**:
  - Verify model name is correct
  - Check internet connection
  - Ensure you have access to the model (some require approval)
  - Try a different model from the supported list
</Accordion>

<Accordion title="Slow training">
  **Problem**: Training is taking too long.
  
  **Solutions**:
  - Enable distributed training: `distributed=True`
  - Use a GPU if available
  - Reduce environment complexity
  - Enable auto-scaling for production workloads
</Accordion>

<Accordion title="Port already in use">
  **Problem**: Monitoring dashboard won't start.
  
  **Solutions**:
  - Change the port: `monitoring_port=8001`
  - Kill existing processes: `pkill -f artemis`
  - Use a different terminal/session
</Accordion>
</AccordionGroup>

<Warning>
For production deployments, always configure proper authentication, SSL certificates, and firewall rules for the monitoring dashboard.
</Warning>

## Getting Help

If you encounter issues not covered here:

1. Check the [full documentation](/artemis/introduction)
2. Search [GitHub Issues](https://github.com/noema-research/artemis-rl-gym/issues)
3. Join our [Discord community](https://discord.gg/artemis-rl)
4. Review the [examples](/artemis/examples) for similar use cases